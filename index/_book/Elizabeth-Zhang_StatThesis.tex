%===========================================================
% This is the thesis template for the Statistics major at
% Amherst College. Brittney E. Bailey (bebailey@amherst.edu)
% adapted this template from the Reed College LaTeX thesis
% template in January 2019 with major updates in April 2020.
% Please send any comments/suggestions: bebailey@amherst.edu

% Most of the work for the original document class was done
% by Sam Noble (SN), as well as this template. Later comments
% etc. by Ben Salzberg (BTS). Additional restructuring and
% APA support by Jess Youngberg (JY). Email: cus@reed.edu
%===========================================================

\documentclass[12pt, twoside]{amherstthesis}
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs} %setspace loaded in .cls
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{float}
\floatplacement{figure}{H}
\usepackage{rotating}
\usepackage{fancyvrb}
% User-added packages:
	\usepackage{amsmath}
	\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
% End user-added packages

%===========================================================
% BIBLIOGRAPHY FORMATTING

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the
% following two lines to use the new biblatex-chicago style,
% for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


%===========================================================
% HYPERLINK FORMATTING

% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% End of CII addition
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

%===========================================================
% CAPTION FORMATTING

% Added by CII
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

%===========================================================
% TITLE FORMATTING

\renewcommand{\contentsname}{Table of Contents}

\usepackage{titlesec}
%%%%%%%%
% How to use titlesec:
% \titleformat{⟨command⟩}[⟨shape⟩]{⟨format⟩}{⟨label⟩}{⟨sep⟩}
%  {⟨before-code⟩}[⟨after-code⟩]
%%%%%%%%

\titleformat{\chapter}[hang]
{\normalfont%
    \Large% %change this size to your needs for the first line
    \bfseries}{\chaptertitlename\ \thechapter}{1em}{%
      %change this size to your needs for the second line
    }[]

\titleformat{\section}[hang]
{\normalfont%
    \large % %change this size to your needs for the first line
    \bfseries}{\thesection}{1em}{%
     %change this size to your needs for the second line
    }[]

\titleformat{\subsection}[hang]
{\normalfont%
    \normalsize % %change this size to your needs for the first line
    \bfseries}{\thesubsection}{1em}{%
     %change this size to your needs for the second line
    }[]

% \titleformat{\section}[display]
% {\normalfont%
%     \large% %change this size to your needs for the first line
%     \bfseries}{\chaptertitlename\ \thechapter}{20pt}{%
%     \normalsize %change this size to your needs for the second line
%     }


%===========================================================
% DOCUMENT FONT

% \usepackage{times}
% other fonts available eg: times, bookman, charter, palatino


%===========================================================
% PASSING FORMATS FROM RMD --> LATEX

%%%%%%%%
% NOTE: Dollar signs pass parameters between YAML inputs
% in index.Rmd and LaTeX
%%%%%%%%

\Abstract{
The abstract should be a short summary of your thesis work. A paragraph is usually sufficient here.
}

\Acknowledgments{
Use this space to thank those who have helped you in the thesis process (professors, staff, friends, family, etc.). If you had special funding to conduct your thesis work, that should be acknowledged here as well.

This work was performed in part using high-performance computing equipment at Amherst College obtained under National Science Foundation Grant Number 2117377. The data used for simulations in this thesis was supported by the National Institute of Environmental Health Sciences of the National Institutes of Health under Award Numbers U2CES026555 and U2CES026553. The content is solely the responsibility of the author and does not necessarily represent the official views of the National Science Foundation or the National Institutes of Health.
}

\Dedication{

}

\Preface{

}

% Formatting R code display
% Syntax highlighting #22
  \usepackage{color}
  \usepackage{fancyvrb}
  \newcommand{\VerbBar}{|}
  \newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
  % Add ',fontsize=\small' for more characters per line
  \usepackage{framed}
  \definecolor{shadecolor}{RGB}{248,248,248}
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
  \newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
  \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\BuiltInTok}[1]{#1}
  \newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
  \newcommand{\ExtensionTok}[1]{#1}
  \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\ImportTok}[1]{#1}
  \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\NormalTok}[1]{#1}
  \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\RegionMarkerTok}[1]{#1}
  \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

% Formatting R code: set baselinestretch = 1.5 for double-spacing
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{
  baselinestretch = 1,
  commandchars=\\\{\}}

% Formatting R output display: set baselinestretch = 1.5 for double-spacing
\DefineVerbatimEnvironment{verbatim}{Verbatim}{
  baselinestretch = 1,
  % indent from left margin
  xleftmargin = 1mm,
  % vertical grey bar on left side of R output
  frame = leftline,
  framesep = 0pt,
  framerule = 1.5mm, rulecolor = \color{black!15}
  }

\title{Flexible Bayesian Regression Models for Quantifying Complex Interactions in Exposure Mixture Studies}
\author{Elizabeth Zhang}
\date{April DD, 20YY}
\division{}
\advisor{Amy Wagaman}
% for second advisor
\institution{Amherst College}
\degree{Bachelor of Arts}
\department{Mathematics and Statistics}

% Fix from pandoc about cslreferences?
% https://github.com/mpark/wg21/issues/54
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}[2]%
  {}%
  {\par}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

% ===========================================
% DOCUMENT SPACING

\setlength{\parskip}{0pt}
% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


% ===========================================
% ===========================================
% ===========================================
\begin{document}

\doublespace
% Everything below added by CII
  \maketitle

\frontmatter % this stuff will be roman-numbered
\pagenumbering{roman}
\pagestyle{fancyplain}
%\pagestyle{fancy} % this removes page numbers from the frontmatter

  \begin{abstract}
    The abstract should be a short summary of your thesis work. A paragraph is usually sufficient here.
  \end{abstract}
  \begin{acknowledgments}
    Use this space to thank those who have helped you in the thesis process (professors, staff, friends, family, etc.). If you had special funding to conduct your thesis work, that should be acknowledged here as well.

    This work was performed in part using high-performance computing equipment at Amherst College obtained under National Science Foundation Grant Number 2117377. The data used for simulations in this thesis was supported by the National Institute of Environmental Health Sciences of the National Institutes of Health under Award Numbers U2CES026555 and U2CES026553. The content is solely the responsibility of the author and does not necessarily represent the official views of the National Science Foundation or the National Institutes of Health.
  \end{acknowledgments}

  \hypersetup{linkcolor=black}
  \setcounter{tocdepth}{2}
  \tableofcontents

  \addcontentsline{toc}{chapter}{List of Tables}\listoftables

  \addcontentsline{toc}{chapter}{List of Figures}\listoffigures


\mainmatter % here the regular arabic numbering starts
\pagenumbering{arabic}
\pagestyle{fancyplain} % turns page numbering back on

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

Rapid industrial development has created conditions of cumulative chronic toxicity which pose an acute risk to the wellbeing of humans and our living environment. In fact, it has been estimated that, globally, human activity releases chemicals at a rate of 220 billion tons per annum (Cribb, 2016). These staggering levels of pollution have led scholars to formally declare that humanity has surpassed the safe operating space of the planetary boundary for novel entities (Persson et al., 2022). As a result, exposure to low levels of pollutants has become an inevitable peril of daily life (Naidu et al., 2021; Vineis, 2018). In this new era of pervasive toxicity, understanding the nature and severity of health effects associated with chemical exposures is especially timely.

For this, we turn to epidemiological studies. The broad field of preventive epidemiology involves the identification of potentially modifiable risk factors that contribute to the burden of disease within human populations. Environmental epidemiology, in particular, considers the effect of environmental exposures --- chemical or otherwise. However, studies concerning chemical pollutants in environmental epidemiology have historically focused on elucidating the effect and mechanisms of exposures to a single pollutant. In reality, humans are invariably exposed to numerous complex exposure mixtures which together contribute to the progression of adverse health outcomes. Therefore, risk assessments of single pollutants likely fail to capture the true consequences of these complex exposures (Heys, Shore, Pereira, Jones, \& Martin, 2016). Assessing mixtures of chemicals can also have more direct implications for public health interventions. The United States Environmental Protection Agency (U.S. EPA) currently passes regulations for individual pollutants. In practice, though, regulation occurs by controlling the source of pollution, which is responsible for the production of a whole mixture of chemicals with specific joint effects on human health. As a result, the National Academies of Science has advocated for a multipollutant regulatory approach, which is likely to be more protective of human health (National Academies of Sciences, Engineering, and Medicine, Division on Earth and Life Studies, Board on Environmental Studies and Toxicology, \& Committee on Incorporating 21st Century Science into Risk-Based Evaluations, 2017).

There are clear practical motivations for studies that examine the health effects of exposure to co-occurring mixtures of chemicals, hereafter referred to as exposure mixtures. However, expanding the focus of analysis from one exposure to multiple exposures introduces unique statistical challenges. In addition to a common issue of small effect sizes and small sample sizes present in most exposure analyses, multiple exposure analyses must also contend with high-dimensionality, collinearity, non-linear effects, and non-additive interactions (Yu et al., 2022). In particular, data with numerous pollutants, or predictors, require exponentially greater levels of complexity and time cost in analysis. Collinearity between exposures is common when analyzing pollutants from a single source and can lead to unstable estimates in a generalized linear model if left unaccounted for. Finally, exposures can have both non-linear single effects and non-additive interaction effects, which are difficult to capture unless explicitly specified in the model.

The classic multiple linear regression framework often fails to capture the true effects in this setting. In the past few years, a wide variety of statistical methods have been developed to overcome these challenges (see reviews at Gibson et al., 2019; Yu et al., 2022), which have been accompanied by a host of comparative simulation studies for general mixture scenarios (e.g., Hoskovec, Benka-Coker, Severson, Magzamen, \& Wilson, 2021; Lazarevic, Knibbs, Sly, \& Barnett, 2020; Pesenti et al., 2023). However, to our knowledge, there has yet to be a simulation study which provides conclusive guidance about the ability of recently developed methods to conduct inference on non-additive interactions between exposures when the nature and effect sizes of these interactions vary.

The goal of this thesis is to fill this gap in the literature by exploring the theory and performance of Bayesian regression techniques for quantifying complex interactions between multiple environmental exposures and related covariates. Specifically, we will compare two recently developed models for estimating the health effects of exposure mixtures: Bayesian Kernel Machine Regression (BKMR) (Bobb et al., 2015) and Bayesian Semiparametric Regression (BSR) (Antonelli et al., 2020).

In an age where anthropogenic actions have radically reshaped the earth, humanistic inquiry can offer critical insights into how we navigate the hazards of our rapidly changing environment. We begin in Chapter 2 by contextualizing this thesis with a brief overview of cultural and social understandings of toxicity. Chapter 3 explains the motivation for studying interactions and provides background on the theory of Bayesian methods for analyzing exposure mixtures. Chapter 4 assesses the performance of these methods using a simulation study, based on a dataset with information on the relationship between prenatal exposure to heavy metals and gestational weight. We conclude with a discussion of the implications of this work for the future study of complex interactions in exposure mixture studies.

\hypertarget{humanistic}{%
\chapter{Humanistic Perspective}\label{humanistic}}

List of loose ideas:
\begin{itemize}
\tightlist
\item
  goal of modernism: define single causes for single effects
  \begin{itemize}
  \tightlist
  \item
    study chemicals in isolation to obtain purely mechanistic explanation of their toxicity
  \end{itemize}
\item
  contrast with idea of relationality: entities cannot be understood without considering their \emph{relationality} to other surrounding entities
  \begin{itemize}
  \tightlist
  \item
    relationality disrupts the notion of bounded objects
  \item
    motivation for chemical mixtures: chemicals themselves are not independent from surrounding chemicals
  \item
    motivation for chemical mixtures in the context of social epidemiology: the effects of chemicals are modulated by structural/social conditions
  \end{itemize}
\item
  relationality also disrupts Cartesian split between body and mind
  \begin{itemize}
  \tightlist
  \item
    racial hierarchy positions certain groups closer to the bounds of the corporal body, while other groups have transcended these bounds and are defined by their intellect (i.e., the mind)
  \item
    result --\textgreater{} some bodies are seen as inherently more susceptible to chemical exposure, more ``porous''
  \item
    leads to damage centered research which, while well-intentioned, inadvertently de-humanizes marginalized groups
  \end{itemize}
\item
  remedy: relationality leads into concept of alterlife, modern life is inseparable from alteration due to chemical exposure
\end{itemize}
\hypertarget{bayes}{%
\chapter{Bayesian Regression Methods}\label{bayes}}

\hypertarget{motivation}{%
\section{Motivation}\label{motivation}}

We are interested in using Bayesian regression techniques to characterize the nature of non-additive interactions in exposure mixture studies. We begin by reviewing definitions for what constitutes an interaction and why interactions are relevant from public health and biological relevance.

\hypertarget{interactions-from-a-statistical-perspective}{%
\subsection{Interactions from a statistical perspective}\label{interactions-from-a-statistical-perspective}}

First, we define additivity and non-additivity in the traditional statistical paradigm (Siemiatycki \& Thomas, 1981). Suppose we have two variables \(x_1\) and \(x_2\), and we want to consider their effect on some outcome of interest. If specifying {[}effect due to \(x_1\) and \(x_2\){]} = {[}effect due to \(x_1\){]} + {[}effect due to \(x_2\){]} can adequately capture this relationship, then we say that \(x_1\) and \(x_2\) each have an \textbf{additive effect} on the outcome and that there is no interaction between them. On the other hand, if there is variability in the outcome that can be captured by an additional term equal to some function of \(x_1\) and \(x_2\), we say that there is a \textbf{non-additive interaction} between \(x_1\) and \(x_2\). In this case, {[}effect due to \(x_1\) and \(x_2\){]} = {[}effect due to \(x_1\){]} + {[}effect due to \(x_2\){]} + {[}effect due to \(f(x_1, x_2)\){]}, where \(f\) is a non-zero function.

For our sake, when we refer to ``interaction,'' we mean any non-additive interaction. We consider such non-additive interactions to be complex, meaning that they are difficult to detect. To see why, let us consider running a linear regression for \(Y\) on \(x_1\) and \(x_2\). The theoretical model would be defined as

\[
Y = \beta_0+\beta_1x_1+\beta_2x_2 + \beta_{12}f(x_1, x_2),
\]

\noindent where the \(\beta\)'s represent the effect sizes. We can see that the form of the interaction must be explicitly specified in the formulation of the model. Most commonly, a multiplicative interaction is assessed, where \(f(x_1, x_2) = x_1*x_2\). However, a non-additive interaction can take on many different forms, the true nature of which is difficult to determine analytically.

We used a two-predictor case above, but interactions can also exist between more variables (i.e., two-way by \(f(x_1, x_2)\), three-way by \(f(x_1, x_2, x_3)\), etc.). So, if we wanted to assess all possible interactions, the number to consider quickly becomes intractable in high-dimensional settings. For instance, consider modelling 10 predictors in the above linear regression setting. In order to be assessed, each interaction must be explicitly specified as a new term in the model. Even if we only considered one form for each interaction, including all possible two-way interactions would involve adding \({10 \choose 2} = 45\) additional terms to the model, and all possible three-way interactions would add \({10 \choose 3} = 120\) additional terms.

It is important also to acknowledge, here, that there is a limit to how many variables can be included in an interaction before it becomes incomprehensible to most humans. For instance, Halford, Baker, McCredden, \& Bain (2005) suggest that there is a steep decline in interpretability from three- to four-way interactions, and that five-way interactions are only interpreted correctly at chance level (Halford et al., 2005). Hence, for practical purposes, we will limit our exploration to two- and three-way interactions.

\hypertarget{mechanistic-and-public-health-relevance}{%
\subsection{Mechanistic and public health relevance}\label{mechanistic-and-public-health-relevance}}

Thus far, we have discussed interactions within a statistical paradigm. However, in addition to being an interesting estimation challenge, non-additive interactions are also relevant in exposure mixture studies from both a mechanistic and public health point of view.

From a mechanistic perspective, a non-additive statistical interaction between two chemical exposures suggests that these compounds may be functionally interacting with each other. Theoretical models propose that such interactions can be classified as either synergistic or antagonistic (Heys et al., 2016; Plackett \& Hewlett, 1952). In a synergistic interaction, the joint effects of a mixture exceed the independent effects of each component. This usually occurs if a chemical induces an enzyme involved with the activation of a second chemical or if a chemical inhibits an enzyme that would have otherwise degraded a second chemical. For example, it has been shown that organophosphates slow the degradation of pyrethoids by inhibiting detoxifying enzymes --- these two classes of chemicals are often found together in commercial insecticide mixtures (Hernández et al., 2013).

On the other hand, in an antagonistic interaction, the joint effects of a mixture are less than their independent effects. This can occur either through competition at the target site of an enzyme or through direct chemical reactions with each other. In general, synergistic interactions are more concerning in risk assessments, as they lead to underestimation of the true toxicity of a mixture.

It should be noted, though, that while statistical interactions may provide some insight into how exposure mixtures are related to health, they cannot confirm their underlying biology (VanderWeele \& Knol, 2014). If the goal is to assess a meaningful biological interaction, then the discovery of a statistical interaction should be followed up by a functional study.

Now, from a public health perspective, we might be interested in how exposure mixtures interact with other covariates, or, in other words, how social and health factors might mediate the relationship between a health outcome and chemical exposures (VanderWeele \& Knol, 2014). In our case, we can include these additional covariates in the exposure mixture model, where, statistically, they would contribute to the model in the same manner as another chemical exposure: a predictor. A statistical interaction in our model between a covariate and an exposure would indicate that the \emph{magnitude} of the effect of reducing the level of an exposure might differ across various levels of the covariate. This finding could be relevant to public health policy makers, as the potential benefit of regulating a pollutant might differ across groups. For instance, it has been suggested that nutritional intake may modify susceptibility to chemical exposures (e.g., Kannan, Misra, Dvonch, \& Krishnakumar, 2006; Kordas, Lönnerdal, \& Stoltzfus, 2007).

In many cases, we might assess a covariate related to health inequity, such as socioeconomic status. We provide a cautionary comment, here, that an interaction term should not be the \emph{sole} measure used to measure a health disparity (Ward et al., 2019). In this case, we should first consider the independent, additive association between the covariate and levels of exposure or rates of a health outcome, in order to contextualize the meaning of a potential interaction term.

\hypertarget{bkmr}{%
\section{Bayesian kernel machine regression (BKMR)}\label{bkmr}}

In this section, we introduce the theory of BKMR. First, we define the notation that we will be using for kernel machine regression:
\begin{itemize}
\tightlist
\item
  \(X_m\) is an exposure in the exposure matrix \(\textbf{X}\) with \(m = 1, \dots, M\)
\item
  \(\textbf{x}_i\) is a vector of values for a single observation in \(\textbf{X}\) with \(i = 1, \dots, n\)
\item
  \(x_{im}\) is the \(i\)th observation of \(X_m\)
\item
  \(\textbf{z}_i\) is a vector of covariates for a single observation in the matrix \(\textbf{Z}\), which contains a set of covariates, with \(i = 1, \dots, n\)
\item
  \(Y_i\) is an observation of \(\textbf{Y}\), measuring the health outcome in this case
\item
  \(h(\cdot)\) is the flexible function relating \(\textbf{x}\) to \(\textbf{Y}\)
\item
  \(k\) is the kernel function, the Gaussian in this case
\item
  \(\textbf{K}\) is the \(n \times n\) kernel matrix, with \((i, j)\)th element \(k(\textbf{x}_i, \textbf{x}_j)\)
\item
  \(\rho\) is the parameter which controls smoothnessm, associated with the kernel function
\item
  \(\tau\) is the parameter multiplied by the kernel matrix to relate \(\textbf{K}\) to \(h\)
\item
  \(\boldsymbol{\beta}_{\textbf{z}}\) is a vector of the weights on the covariates, and
\item
  \(\boldsymbol\varepsilon_i \overset{\mathrm{iid}}{\sim} N(0, \sigma^2)\) are the residuals of the response.
\end{itemize}
And, we define the notation that we will be using specific to BKMR:
\begin{itemize}
\tightlist
\item
  \(r_m=1/\rho_m\) is an augmented variable in \(\textbf{r}\) in the kernel matrix, controlling smoothness
\item
  \(\delta_m\) is an indicator variable in \(\boldsymbol\delta\) which represents inclusion in the model
\item
  \(\mathcal{S}_g\) is a group of partitioned predictors with \({g=1,\dots,G}\)
\item
  \(\{\delta_m|\textbf{x}_m \in{\mathcal{S}_g}\}\) is an indicator variable in \(\boldsymbol\delta_{\mathcal{S}_g}\) which represents inclusion of a parameter in group \(g\) in the model
\item
  \(\pi\) is the prior probability of inclusion of a predictor in the model, and
\item
  \(\lambda \equiv \tau\sigma^{-2}\) is a convenient way to define the prior on \(\tau\).
\end{itemize}
\hypertarget{kernel-machine-regression}{%
\subsection{Kernel machine regression}\label{kernel-machine-regression}}

We begin by introducing kernel machine regression, with attention to its specific implementation in BKMR. First proposed by Nadaraya (1964) and Watson (1964), kernel machine regression is a nonparametric regression technique that can be used to capture non-linear effects and non-additive interactions. In this introduction, we follow the presentation of kernel machine regression provided by Bobb et al. (2015).

To contextualize this method, we start at the typical linear regression setting,

\[
Y_i = \textbf{x}_i^\top \boldsymbol{\beta}_{\textbf{x}} + \textbf{z}_i^\top \boldsymbol{\beta}_{\textbf{z}} + \varepsilon_i,
\]

\noindent where \(Y_i\) measures a health outcome at a given point, \(\textbf{x}_i = [x_{i1},\dots,x_{iM}]\) is a vector of M exposures, \(\textbf{z}_i\) is a vector of covariates, \(\boldsymbol{\beta}_{\textbf{x}}\) and \(\boldsymbol{\beta}_{\textbf{z}}\) are vectors of weights for the exposures and covariates, respectively, and \(\varepsilon_i\) is a random variable from \(\boldsymbol\varepsilon \overset{\mathrm{iid}}{\sim} N(0, \sigma^2)\). We can see that this function assumes that there is a linear relationship between the exposure and the response, and that the combined effects of multiple exposures are additive.

Kernel machine regression defines this relationship using a flexible function \(h: \mathbb{R}^M \rightarrow \mathbb{R}\), where

\[
Y_i = h(\textbf{x}_i) + \textbf{z}_i^\top \boldsymbol{\beta}_{\textbf{z}} + \varepsilon_i.
\]

\noindent Here, \(h(\cdot)\) is represented by the function \(k(\cdot, \cdot)\), a kernel. The kernel controls the covariance, or the similarity, between values of \(h(\textbf{x})\) and as such ensures that points near each other on the prediction surface will have similar values --- or, in other words, that the prediction surface will be smooth. In the case of kernel machine regression, we define a positive definite kernel where \(k: \mathbb{R}^M\times \mathbb{R}^M \rightarrow \mathbb{R}\). Note also that covariates are assumed to have a linear, non-additive effect on the response.

There are many choices of functions for \(k\). BKMR uses the Gaussian kernel, also known as the radial basis function or, sometimes, the squared exponential kernel. The Gaussian kernel is defined as

\[
k(\textbf{x}, \textbf{x}') = \textrm{exp}\bigg\{
-\frac{||\textbf{x}-\textbf{x}'||^2}{\rho} \bigg\},
\]

\noindent where \(||\textbf{x}-\textbf{x}'||^2 = \sum_{m=1}^M{(x_{m}-x_{m}')^2}\) for a set of exposure values \(\textbf{x}\) and the exposure values of another subject \(\textbf{x}'\), and \(\rho\) is a tuning parameter that controls the relationship between the correlation between two points and their distance. Greater values of \(\rho\) will enforce more dependence between points and make the resulting function smoother. \(h\) is related to \(k\) by a multiplicative constant \(\tau\), a tuning parameter which controls the vertical scale of \(h\).

Now that we have defined \(h\) and \(k\), we can think about how to characterize the relationship between our response and exposures Kernel machine regression is a nonparametric technique because it does not specify a functional form for this relationship. Hence, we will think about estimating the response at a particular query point. Operationally, Müller (1987) demonstrated that kernel machine regression uses a weighted average of all the observations in the dataset to estimate the response, defined as

\[
\bar{Y} = \frac{\sum_{i=1}^nw_iY_i}{\sum_{i=1}^nw_i},
\]

\noindent with some set of weights \(\{w_i\}_{i=1}^n\). Intuitively, we want to weight the observations that are closer to the query point more heavily. Using the Gaussian kernel as a weight allows us to achieve this. Replacing the weight with the Gaussian kernel, we get

\[
\bar{Y} = \frac{\sum_{i=1}^n k(\textbf{x}, \textbf{x}_i) Y_i}
{\sum_{i=1}^n k(\textbf{x}, \textbf{x}_i)}.
\]

As we move through the predictor space, we can think of the prediction as a continuous moving average of local points in the dataset. The correlation between two values of \(h\) is defined as

\[
\textrm{cor}(h_i, h_j) = \textrm{exp} \bigg\{-\frac{||
\textbf{x}_{i}-\textbf{x}_{j}||^2}{\rho} \bigg\},
\]

\noindent which allows us to see that values of \(h\) near each other will have a higher correlation and thus similar values. This is also why the resulting function is smooth.

\hypertarget{connection-to-mixed-models}{%
\subsection{Connection to mixed models}\label{connection-to-mixed-models}}

It is useful to make connections between this definition of kernel machine regression and mixed models. Liu, Lin, \& Ghosh (2007) demonstrated this by representing \(h(\textbf{x})\) as following a Gaussian process probability distribution,

\[
h(\textbf{x}) \sim \mathcal{GP}(\textbf{0}, \tau k(\textbf{x}, \textbf{x}')),
\]

\noindent with covariance function \(k\), where \(\textbf{x}\) is a vector of the exposure values, and \(\textbf{x}'\) contains the exposure values of another subject. A Gaussian process is a collection of random variables, of which any finite number follow a multivariate normal distribution (Schulz, Speekenbrink, \& Krause, 2018). Here, we assume that the expected value of the \(h\) function with input \(\textbf{x}\) is \(\textbf{0}\). We use \(k\) for the covariance function, which represents the dependence between the function values with inputs \(\textbf{x}\) and \(\textbf{x}'\): \(k(\textbf{x}, \textbf{x}') = \mathbb{E}[(h(\textbf{x})- \textbf{0}) (h(\textbf{x}')- \textbf{0})]\).

Now, we can represent \(h\) as a collection of variables from a Gaussian process. \(h\) follows a multivariate normal distribution,

\[
h({\textbf{x}}) \sim N(\textbf{0}, \tau\textbf{K}),
\]

\noindent where \(h({\textbf{x}}) = [h(\textbf{x}_1), h(\textbf{x}_2), \dots, h(\textbf{x}_n)]^\top\) and \(\textbf{K}\) is the kernel matrix. The kernel matrix is an \(n \times n\) matrix with \((i, j)\)th element \(k(\textbf{x}_i, \textbf{x}_j)\). Now, returning back to the regression view, we can think of each \(Y_i\) as following the distribution,

\[
Y_i \overset{\mathrm{ind}}{\sim} N(h(\textbf{x}_i) + \textbf{z}_i^\top \boldsymbol{\beta}_{\textbf{z}}, \sigma^2) \text{ for } i = 1,\dots,n,
\]

\noindent where \(\sigma^2\) comes from the variance of the residuals. Here, \(h\) can be interpreted as a random effect.

\hypertarget{bkmrtoy}{%
\subsection{Toy example}\label{bkmrtoy}}

In the following section, we illustrate kernel machine regression with a toy example.

Consider the following case where we want to model the relationship between a single predictor and a response variable. Suppose the true relationship between \(x\) and \(Y\) is defined \(Y = e^{\frac{x}{10}} + 2\sin(\frac{x}{2})\). We simulate 51 equally spaced observations of \(x\) from 0 to 25, with error \(\varepsilon_i \overset{\text{iid}}\sim N(0, 0.25)\).
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch3_toy1} 

}

\caption{Non-linear data with a true relationship (orange) and a fitted linear regression (blue).}\label{fig:toy1}
\end{figure}
Figure \ref{fig:toy1} illustrates the shape of our simulated non-linear data and the fit proposed by a simple linear regression. We can observe that the linear regression fails to capture the true non-linear relationship. In this case, this would lead to an underestimation of the true association between \(x\) and \(Y\). Now, we will try to capture this relationship using kernel machine regression.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch3_toy2} 

}

\caption{A query point of 12.5 and the weights of neighboring observations based on a Gaussian kernel}\label{fig:toy2}
\end{figure}
To visualize how kernel machine regression works as a moving weighted average, we can consider a query point of 12.5. Figure \ref{fig:toy2} identifies the query point and assigns corresponding weights to the neighboring points based on a normal distribution, which shares the same density as the Gaussian kernel. In this case, we will specify \(\rho = 2\), which is synonymous with assigning the weights using a normal distribution with \(\sigma^2=1\). We can see how an appropriate estimate for \(h(12.5)\) can be obtained by taking a weighted average of the \(Y\)'s, with those observations nearby weighted the most heavily.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch3_toy3} 

}

\caption{Fitted kernel machine regression (blue) with $\rho=2$ compared to the true relationship (orange).}\label{fig:toy3}
\end{figure}
Now, we fit a kernel machine regression on this data with \(\rho=2\) using the \texttt{stats} package in R. We can see in Figure \ref{fig:toy3} that kernel machine regression captures the complex non-linear relationship between \(Y\) and \(x\) and closely follows the true relationship. We do note, though, that the estimation is less precise at the tails, where there is less information provided by local observations. We can also use this example to consider the effect of various values of \(\rho\) on the smoothness of the \(h\) function.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/ch3_toyrho} 

}

\caption{Fitted kernel machine regression with $\rho=0.02$ and $\rho=50$.}\label{fig:toyrho}
\end{figure}
Figure \ref{fig:toyrho} demonstrates the effect of relatively smaller and larger values of \(\rho\) on \(h\). Decreasing the value of \(\rho\) allows kernel machine regression to overfit to the noise in the data by relaxing the dependence of neighboring values of \(h\) to each other. On the other hand, increasing the value of \(\rho\) enforces more dependence in \(h\) and as such results in an underfit estimation. Hence, the choice of \(\rho\) has a strong effect on the performance of kernel machine regression.

\hypertarget{variable-selection}{%
\subsection{Variable selection}\label{variable-selection}}

Now that we have defined kernel machine regression, we can extend it to the Bayesian paradigm. Bobb et al. (2015) showed that the Bayesian approach can outperform frequentist kernel machine regression because simultaneous variable selection and estimation can better capture the exposure-response relationship. In this section, we discuss the two methods for Bayesian variable selection in BKMR: hierarchical variable selection and component-wise variable selection (Bobb et al., 2015).

In order to perform variable selection, we define a parameter that puts a weight on each exposure. Each weight controls the degree to which its associated exposure contributes to the model. In component-wise selection, we do this by augmenting the kernel function as

\[
k(\textbf{x}, \textbf{x}'|\textbf{r}) = \text{exp}\bigg\{ -\sum_{m=1}^Mr_m(x_m-x_m')^2 \bigg\},
\]

\noindent where \(\textbf{r}=[r_1,\dots,r_M]^\top\). We define \(r_m=1/{\rho_m}\), the inverse of the tuning parameter \(\rho_m\) for each \(\textbf{x}_m\). Now, we can imagine that an exposure that is not closely associated with the response will be be assigned a value of \(r_m\) close to 0, which corresponds to a larger value of \(\rho_m\). This larger value of \(\rho_m\) means that this exposure would contribute less to the exposure-response relationship, as depicted in the second panel of Figure \ref{fig:toyrho}.

We now define the kernel matrix \(\textbf{K}_{\textbf{X},\textbf{r}}\) as the \(n\times n\) matrix with \((i,j)\)th element \(k(\textbf{x}, \textbf{x}'|\textbf{r})\). To allow \(r_m\) to equal 0 with non-zero probability, we first define an indicator variable determining whether or not a predictor is included in the variable, which is distributed as

\[
\delta_m \sim \text{Bernoulli}(\pi),
\]

\noindent where \(\pi\) is the prior probability of inclusion. Now, we can assume a ``slab-and-spike'' prior on \(r_m\), distributed as

\[
r_m|\delta_m \sim \delta_mf(r_m) + (1-\delta_m)P_0,
\]

\noindent where \(f(\cdot)\) is some pdf with support \(\mathbb{R}^+\), and \(P_0\) denotes the density with point mass at 0.

While this process of component-wise variable selection works well in a typical multiple regression setting, it can lead to unreliable estimates in situations where the exposures are highly correlated with each other, which is common in exposure mixture studies. In this case, the correlated components contribute similar information to the model, and component-wise variable selection is not able to distinguish which exposure is important. BKMR deals with this problem by introducing hierarchical variable selection.

Hierarchical variable selection involves partitioning the predictors \(\textbf{x}_1, \dots, \textbf{x}_M\) into \(G\) groups, denoted \(\mathcal{S}_g\) with \(g = 1,\dots,G\). These groups should be selected by the user based on prior knowledge, with the aim of keeping within-group correlation high and between-group correlation low. For instance, consider a situation with 4 chemicals, Hg, Pb, As, and Sn. If Hg, Pb, and As were strongly correlated with each other and each weakly correlated with Sn, we might define \(\mathcal{S}_1=\{\textrm{Hg, Pb, As}\}\) and \(\mathcal{S}_1=\{\textrm{Sn}\}\).

The indicators from \(r_m|\delta_m\) are now distributed as
\begin{gather*}
\boldsymbol\delta_{\mathcal{S}_g} | \omega_g \sim 
\text{Multinomial}(\omega_g, \boldsymbol\pi_{\mathcal{S}_g}), g=1,\dots,G,\\
\omega_g \sim\text{Bernoulli}(\pi),
\end{gather*}
\noindent where \(\boldsymbol\delta_{\mathcal{S}_g}=\{\delta_m|\textbf{x}_m \in{\mathcal{S}_g}\}\) and \(\boldsymbol\pi_{\mathcal{S}_g}\) are vectors of indicator variables and prior probabilities, respectively, of a exposure \(\textbf{x}_m\) in group \(\mathcal{S}_g\) entering the model. By this approach, at most one exposure in each group is allowed to enter the model.

While hierarchical variable selection resolves the issue of multicollinearity, it requires specifying subgroups of predictors a priori and assumes that one exposure in each group can capture the information of the rest. Hence, care should be taken to justify the partitioning of exposures when taking this approach.

Note also that the posterior means of \(\delta_m\) generated from these variable selection procedures represent the posterior probability of inclusion of \(\textbf{x}_m\). We can interpret these posterior inclusion probabilities (PIPs) as measures of the relative importance of each exposure These measures can be used to understand the contribution of each exposure to the health outcome of interest in the model.

\hypertarget{prior-specification}{%
\subsection{Prior specification}\label{prior-specification}}

In this section, we specify the default prior distributions and parameters used by the BKMR algorithm (Bobb et al., 2015).

BKMR, by default, assumes \(\rho_m=1/r_m \sim \text{Unif}(a_r,b_r)\), a flat prior between \(a_r\) and \(b_r\) for which the default values are 0 and 100, respectively (Bobb, 2017a). This defines the prior probability of \(\rho\) as equally distributed across any value from 0 to 100. This inverse of this prior corresponds to the slab component of the ``slab-and-spike'' prior, where \(r_m|\delta_m \sim \delta_m\text{Unif}^{-1}(a_r, b_r) + (1-\delta_m)P_0\). As a flat prior, this distribution should be chosen when we have no prior knowledge about the smoothness of the exposure-response function, with hyperparameters \(a_r\) and \(b_r\) selected to represent the range of values we expect \(\rho\) to potentially span.

We have seen that the smoothness of a kernel machine regression responds strongly to different values of \(r_m=1/\rho\), and, accordingly, the model fit of BKMR is sensitive to their prior distribution. In general, the PIPs generated from the variable selection procedure are particularly sensitive to this prior, though their relative importance tends to remain stable (Bobb, Claus Henn, Valeri, \& Coull, 2018). As such, the BKMR algorithm also offers the options to define uniform and gamma priors for the \(r_m=1/\rho\).

Moreover, BKMR assumes that the prior probability of including a predictor (\(\delta_m\)) or group of predictors (\(\omega_g\)) in the model is distributed \(\pi \sim \text{Beta}(a_\pi, b_\pi)\). The default hyperparameters are \(a_\pi=b_\pi=1\), which represent a flat, uninformative prior between 0 and 1. When the hierarchical selection approach is applied, equal values for \(\boldsymbol\pi_{\mathcal{S}_g}\), the probabilities of inclusion for each component in group \(\mathcal{S}_g\), are assumed.

Finally, BKMR assumes that the inverse of the variance of the residuals is distributed \(\sigma^{-2} \sim \text{Gamma}(a_\sigma, b_\sigma)\), with default values of \(a_\sigma=b_\sigma=0.001\), and that the vertical scale of \(h\) is parameterized by \(\lambda \equiv \tau\sigma^{-2} \sim \text{Gamma}(a_\lambda, b_\lambda)\), with default values of \(a_\lambda, b_\lambda\) such that the mean and variance of \(\lambda\) are both equal to 10.

\hypertarget{the-mcmc-algorithm}{%
\subsection{The MCMC algorithm}\label{the-mcmc-algorithm}}

Briefly, we discuss the algorithm used to find the solution in the BKMR package (Bobb et al., 2015, 2018), with commentary on its implications for the model fitting process.

BKMR uses a Markov chain Monte Carlo (MCMC) algorithm with a mix of Gibbs and Metropolis-Hastings samplers to estimate the posterior distributions of the parameters. In particular, a Gibbs step is used to update the distribution of \(\sigma^2\) while a Metropolis-Hastings step is used to update the distribution of \(\lambda\). For component-wise and hierarchical variable selection, \((\textbf{r}, \boldsymbol\delta, \boldsymbol\omega)\) are sampled jointly using a Metropolis-Hastings sampling scheme.

While each distribution generated by the Gibbs step is always accepted, the distributions for \(\lambda\) and \(r_m\) generated by the Metropolis-Hastings steps are accepted based on an acceptance rate (Wagaman \& Dobrow, 2021). The standard deviation of the proposal distribution controls the acceptance rate and as such acts as a tuning parameter (Bobb, 2017b). In general, increasing the standard deviation leads to lower acceptance rates. Acceptance rates that are too low lead to slower convergence, but rates that are too high can cause convergence to a non-optimal distribution.

A major computational limitation of BKMR is that at each iteration of the MCMC algorithm, the \(n \times n\) augmented kernel matrix \(\textbf{K}_{\textbf{Z},\textbf{r}}\) must be inverted multiple times. To offset this, BKMR can employ a Gaussian predictive process which involves specifying a set of \(l\) points, or ``knots,'' that are a subset of the predictor space. The vector of predictors can be approximated by projection onto this lower dimensional space, which allows the algorithm to perform inversions on an \(l\times l\) matrix. A general suggestion is to use this approach to speed up the algorithm when \(n\) is large and to specify \(l\approx n/10\) (Bellavia, 2021).

\hypertarget{bsr}{%
\section{Bayesian semiparametric regression (BSR)}\label{bsr}}

In this section, we introduce the theory of BSR. First, we define the notation that we will be using for spline regression:
\begin{itemize}
\tightlist
\item
  \(X_m\) is a predictor variable in the predictor matrix \(\textbf{X}\) with \(m=1,\dots,M\), measuring exposure variables or covariates
\item
  \(\textbf{x}_i\) is a vector of values for a single observation in \(\textbf{X}\) with \(i=1,\dots,n\)
\item
  \(\textbf{z}_i\) is a vector of covariates for a single observation in the matrix \(\textbf{Z}\), which contains a set of covariates, with \(i = 1, \dots, n\)
\item
  \(Y_i\) is an observation of \(\textbf{Y}\), measuring the health outcome in this case
\item
  \(f(\cdot)\) relates \(\textbf{x}_i\) to \(Y_i\) by a set of basis functions, \(b_j(X)\)
\item
  \(\beta_j\) is a weight on the \(j\)th basis function
\item
  \(P\) is the order of the basis expansion
\item
  \(K\) is a set of \(\xi_k\), \(k=1,\dots,K\), interior knots defining \(K+1\) disjoint intervals, and
\item
  \(\boldsymbol\varepsilon_i \overset{\textrm{iid}}\sim N(0,\sigma^2)\) are the residuals of the response.
\end{itemize}
And, we define the notation that we will be using specific to BSR:
\begin{itemize}
\tightlist
\item
  \(\widetilde{X}_m\) is a \(d\)-dimensional basis function expansion of \(X_m\)
\item
  \(\widetilde{X}_{m_1m_2}\) is a \(d^2\)-dimensional basis expansion of the interaction between \(X_{m_1}\) and \(X_{m_2}\)
\item
  \(f^{(h)}(\cdot)\), where \(h=1,\dots,H\) are a set of functions that sum up to \(f(\cdot)\)
\item
  \(\boldsymbol\zeta = \{\zeta_{mh}\}\) is an indicator for whether the \(m\)th predictor is included in the \(h\)th function
\item
  \(\boldsymbol\beta_S^{(h)}\) is a vector of all the coefficients on the predictors in function \(h\)
\item
  \(\sigma^2_{\boldsymbol\beta}\) is the prior variance on the coefficients, and
\item
  \(\boldsymbol\Sigma_{\boldsymbol\beta}\) is a diagonal matrix with the variances of the multivariate slab prior, \(\sigma^2\sigma^2_{\boldsymbol\beta}\), on the diagonals.
\end{itemize}
\hypertarget{spline-regression}{%
\subsection{Spline regression}\label{spline-regression}}

We begin by introducing spline regression, with attention to its specific implementation in BSR. Spline regression is a semiparametric regression technique that can be used to capture non-linear effects. In this introduction, we follow the presentation of spline regression provided by Antonelli et al. (2020), with additional details and explanation from Hastie, Tibshirani, \& Friedman (2009).

BSR uses spline regression to define the regression relationship as

\[
Y_i = f(\textbf{x}_i) + \textbf{z}_i^\top \boldsymbol{\beta}_{\textbf{z}} +\varepsilon_i,
\]

\noindent where \(f\) is defined by a set of basis functions on the exposures, \(\textbf{x}_i\), \(\textbf{z}_i\) and \(\boldsymbol{\beta}_{\textbf{z}}\) are the covariates and their associated weights, and \(\varepsilon_i\) is a random variable from \(\boldsymbol\varepsilon \overset{\mathrm{iid}}{\sim} N(0, \sigma^2)\). BSR uses natural spline bases. In order to understand how these are constructed, we start with a broad definition of basis expansions, before exploring linear, cubic, and then natural spline bases.

We determine the basis expansion by considering a piece-wise function of \(X_m\) with some order \(P\) and some set of \(K\) knots defining \(K+1\) disjoint intervals. BSR places knots at uniformly sized quantiles within the boundaries of \(X_m\). The most commonly used orders are \(P=1,2,\textrm{and } 4\), the constant, linear, and cubic splines, respectively. To begin, let us consider a continuous piece-wise linear spline basis (i.e., \(P=2\)) of a one-dimensional \(X\) with two interior knots. In this case, we use the following four basis functions:

\nointerlineskip

\[
b_1(X)=1, \hspace{0.25in} b_2(X)=X, \hspace{0.25in} b_3(X)=(X-\xi_1)_+, \hspace{0.25in} b_4(X)=(X-\xi_2)_+,
\]

\noindent where \(\xi_1\) and \(\xi_2\) are the two interior knots, and \(t_+\) denotes the positive part. These bases are used to construct the regression model \(f(X)=\sum_{j=1}^4\beta_jb_j(X)\), which requires estimating \(K+P=4\) parameters. We can check the continuity restrictions at the knots by seeing that \(f(\xi_1^-)=\beta_1 + \xi_1\beta_2\) and \(f(\xi_1^+)= \beta_1 + \xi_1\beta_2+ (\xi_1-\xi_1)\beta_3\) are equal, and likewise at the second knot.

Now, in the case of exposure mixtures, we want smoother functions that can capture the non-linear relationship between the response and the predictors. We can achieve this by increasing the order to \(P=4\) and using a cubic spline, with continuous first and second derivatives at the knots. The cubic spline is the lowest-order spline for which knot-discontinuity cannot be detected by the human eye. For example, for one \(X\) with two interior knots, we use the following six basis functions:
\begin{gather*}
b_1(X)=1, \hspace{0.25in}b_2(X)=X, \hspace{0.25in}b_3(X)=X^2, \\
b_4(X)=X^3, \hspace{0.25in} b_5(X)=(X-\xi_1)^3_+, \hspace{0.25in} b_6(X)=(X-\xi_2)^3_+.
\end{gather*}
\noindent Now, the regression model is defined as \(f(X)= \sum_{j=1}^6\beta_jb_j(X)\) and requires estimating \(K+P=6\) parameters. It can be shown that \(f'(\xi_i^-)= f'(\xi_i^+)\) and \(f''(\xi_i^-)= f''(\xi_i^+)\), and so forth.

However, the behavior of polynomials near the boundaries of \(X\), where there is less information, can be erratic. Natural cubic splines, also referred to as just natural splines, address this by imposing an additional restriction of linearity at the boundaries of \(X\). Paradoxically, this also leads to a simpler model with four fewer parameters to estimate. A general definition of the \(K\) basis functions for a natural spline with interior knots \(\xi_j\), \(j=1,\dots,K\), is given by:
\begin{gather*}
b_1(X)=1, \hspace{0.25in} b_2(X)=X, \hspace{0.25in} b_{k+2}(X)=d_k(X)-d_{K-1}(X), \\
d_k(X)=\frac{(X-\xi_k)^3_+ - (X-\xi_K)^3_+}{\xi_K-\xi_k}.
\end{gather*}
\noindent Here, the regression model is defined as \(f(X) = \sum_{j=1}^K\beta_jb_j(X)\), with \(K\) parameters. BSR uses natural splines to specify the regression relationship.

\hypertarget{toy-example}{%
\subsection{Toy example}\label{toy-example}}

In the following section, we illustrate spline regression using the same toy example used to introduce kernel machine regression. See Section \ref{bkmrtoy} and Figure \ref{fig:toy1} for details on the parameters used to generate simulated data.

As in Section \ref{bkmrtoy}, we consider a case where we want to model the relationship between a single exposure and a response variable, where the true relationship between \(x\) and \(Y\) is defined as \(Y = e^{\frac{x}{10}} + 2\sin(\frac{x}{2})\). We fit a series of linear, cubic, and then natural spline regressions to illustrate the general framework of a natural spline regression.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch3_toy4} 

}

\caption{Linear spline regression (blue) with four knots (dotted lines) compared to the true relationship (orange).}\label{fig:toy4}
\end{figure}
Figure \ref{fig:toy4} illustrates the fit proposed by a linear spline regression with order \(P=2\). We can see that the implementation of knots allows for even a linear fit to capture more of the nuances in this nonlinear relationship, as compared to a standard linear regression.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch3_toy5} 

}

\caption{Cubic spline regression (blue) with four knots (dotted lines) compared to the true relationship (orange).}\label{fig:toy5}
\end{figure}
However, this linear spline regression is still unable to fully estimate the nonlinearity in our example. Increasing the order to \(P=4\) with a cubic spline regression offers additional flexibility. Figure \ref{fig:toy5} illustrates the fit proposed by this model. Here, we can see the benefits of using a cubic polynomial relationship in a nonlinear setting: the estimated relationship is continuous at the knots, and the nonlinear relationship has been flexibly captured.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch3_toy6} 

}

\caption{Natural spline regression (blue) with four knots (dotted lines) compared to the true relationship (orange).}\label{fig:toy6}
\end{figure}
Our final modification involves imposing linearity constraints on the boundaries of \(x\) to implement a natural spline regression. Figure \ref{fig:toy6} shows that the fit estimated using a natural spline regression. The fitted line is only slightly different than that proposed by a cubic spline regression in Figure \ref{fig:toy5}. The most noticeable difference is that the slopes of the tails of the natural spline regression are less extreme than for the cubic spline regression.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/ch3_toybounds} 

}

\caption{Natural and cubic spline regression (blue) compared to the true relationship (orange) extrapolated outside the bounds of $x$ (dotted lines).}\label{fig:toybounds}
\end{figure}
Extrapolation outside the scope of \(x\) allows us to see the effect of the linearity constraints imposed by natural regression. Figure \ref{fig:toybounds} demonstrates how the cubic spline regression behaves erratically outside the bounds of \(x\), as the cubic polynomial lines tend toward \(\pm\infty\), while the natural spline regression follows a more appropriate linear trend. As the natural spline regression is more reliable near the boundaries of \(x\) and also simpler to estimate, BSR adopts the use of natural splines.

\hypertarget{model-formulation-in-bsr}{%
\subsection{Model formulation in BSR}\label{model-formulation-in-bsr}}

Now that we have defined natural splines, we introduce BSR, following the presentation in Antonelli et al. (2020). We demonstrate the construction of \(f\) in BSR by first assuming a two-dimensional case with exposures \(X_1\) and \(X_2\). We define
\begin{gather*}
f(X_1) = \widetilde{X}_1\boldsymbol{\beta}_1, \hspace{0.25in} f(X_2) = \widetilde{X}_2\boldsymbol{\beta}_2,\\
f(X_1,X_2) = \widetilde{X}_1\boldsymbol{\beta}_1 + \widetilde{X}_2\boldsymbol{\beta}_2 + \widetilde{X}_{12}\boldsymbol{\beta}_{12},
\end{gather*}
\noindent where \(\widetilde{X}_m=[b_{m1}(X_m),\dots,b_{md}(X_m)]\) represents a \(d\)-dimensional basis function expansion for \(m=1,2\), and \(\widetilde{X}_{12}= [b_{11}(X_1)b_{21}(X_2), b_{11}(X_1)b_{22}(X_2), \dots, b_{1d}(X_1)b_{2d}(X_2)]\) represents a \(d^2\)-dimensional basis expansion of the interaction between \(X_1\) and \(X_2\). \(d\) is an influential tuning parameter. BSR by default assumes that all exposures have the same number of degrees of freedom and uses the Watanabe-Akaike (WAIC) model selection criterion to select \(d\), which approximates leave one out cross validation. Note that we must explicitly model the effect of the interaction term by assuming a multiplicative interaction between the basis functions of the predictors.

Extending to the multi-dimensional setting, BSR assumes the following general model formulation:
\begin{gather*}
f(\textbf{x}_i)= \sum_{h=1}^Hf^{(h)}(\textbf{x}_i), \\
f^{(h)}(\textbf{x}_i)= \sum_{m_1=1}^M\widetilde{x}_{im_1}\boldsymbol\beta_{m_1}^{(h)} + 
\sum_{m_1=2}^M\sum_{m_2<m_1}\widetilde{x}_{im_1m_2}\boldsymbol\beta_{m_1m_2}^{(h)} + \dots,
\end{gather*}
\noindent where \(f^{(h)}(\textbf{x}_i)\) includes a summation of all \(M\)-way interactions. The inclusion of all \(M\)-way interactions makes the model far too overparameterized. Moreover, \(f(\textbf{x}_i)\) is a sum of \(k\) different functions \(f^{(h)}(\textbf{x}_i)\) where a value for \(H\) is selected in order to capture all exposure effects in the model. Each of the \(H\) functions has the same functional form, and so the regression coefficients for a function \(f^{(h)}(\textbf{x}_i)\) are only identifiable up to a constant --- this means that there are multiple sets of coefficients that could be estimated from the same data.

\hypertarget{sparsity-inducing-priors}{%
\subsection{Sparsity inducing priors}\label{sparsity-inducing-priors}}

In order to handle the overparameterization and non-identifiability of the model, BSR implements multivariate sparsity inducing priors. In this section, we follow the presentation provided in Antonelli et al. (2020).

First, we define indicators \(\boldsymbol\zeta=\{\zeta_{mh}\}\) representing whether the \(m\)th exposure is included in the \(h\)th function:
\begin{align*}
P(\zeta_{mh}=1) &= \tau_h^{\zeta_{mh}}(1-\tau_h)^{1-\zeta_{mh}} 
I(A_h\not\subset A_{h'}\forall h'\neq h \textrm{ or } A_h=\{\}),\\
&\textrm{where } A_h=\{m:\zeta_h=1\}.
\end{align*}
\noindent Here, the indicators follow a Bernoulli distribution with prior probability of inclusion \(\tau_h\). The posterior means of \(\boldsymbol\zeta\), i.e.~the PIPs, can be interpreted as measures of relative variable importance. We include an indicator function \(I()\) that represents whether the function \(h\) contains a unique set of predictors. This indicator ensures that no function contains exposures that are a subset of those in another function, \(h'\), in which case this function would be redundant and thus removed from the model entirely.

Now, we assume a multivariate slab-and-spike prior on the regression coefficients:
\begin{align*}
P(\boldsymbol\beta_S^{(h)}|\boldsymbol\zeta) &= \bigg(1-\prod_{m\in S}\zeta_{mh}\bigg)P_{\textbf{0}} +
\bigg(\prod_{m\in S}\zeta_{mh}\bigg) \psi_1(\boldsymbol\beta_S^{(h)}), \\
&\textrm{where } S \textrm{ is some subset of } {1, 2, \dots, m}.
\end{align*}
\noindent Here, \(P_{\textbf{0}}\) denotes the density with point mass at \(\textbf0\), and \(\psi_1()\) is a multivariate normal distribution with mean \(\textbf0\) and covariance \(\boldsymbol\Sigma_{\boldsymbol\beta}\), a diagonal matrix with \(\sigma^2\sigma_{\boldsymbol\beta}^2\) on the diagonals.

\hypertarget{prior-specification-1}{%
\subsection{Prior specification}\label{prior-specification-1}}

In this section, we discuss the priors and their default specifications in BSR (Antonelli et al., 2020).

The priors on \(\boldsymbol\Sigma_{\boldsymbol\beta}\), the diagonal matrix with \(\sigma^2\sigma^2_{\boldsymbol\beta}\) on the diagonals, control the shrinkage of \(\boldsymbol\beta_S^{(h)}\). Variable selection is sensitive to the choice of prior on this parameter, so BSR implements an empirical Bayes strategy to obtain a prior distribution for \(\sigma^2_{\boldsymbol\beta}\) based on the data. While this is not a fully Bayesian approach, it has been shown that this strategy works better in practice (Antonelli et al., 2020). Additionally, the default prior for \(\sigma^2\) is assumed to follow a \(\textrm{Gamma}(0.001, 0.001)\) distribution.

However, when there is a weak relationship between the exposures and relationship, the estimated prior variance for the slab \(\sigma^2_{\boldsymbol\beta}\) can be very small. In this case, the shape of the slab approximates the point mass of 0 at the spike, and the PIPs become difficult to accurately estimate. BSR avoids this by imposing a lower bound on the variance. This is determined by establishing a constant value for \(\tau_h\), the prior probability of inclusion, for all \(h\) and then permuting the rows of \(Y\) (i.e., breaking up the relationship). Then, a grid of values for \(\sigma^2_{\boldsymbol\beta}\) are tested until some predefined threshold of the posterior probability of inclusion is obtained (e.g., 0.25 for a main effect and 0.05 for a two-way interaction). If the empirical Bayes estimate for \(\sigma^2_{\boldsymbol\beta}\) is less than this lower bound, then the lower bound is used instead.

Finally, BSR assumes that \(\tau_h \sim \textrm{Beta}(L,\gamma)\), which defines the prior probability of including a predictor for all functions \(h\). If \(L\) is some predefined constant, and \(\gamma=m\), the number of predictors, then the prior amount of sparsity should increase as the number of predictors increases (Antonelli et al., 2020).

\hypertarget{the-mcmc-algorithm-1}{%
\subsection{The MCMC algorithm}\label{the-mcmc-algorithm-1}}

We also briefly discuss the MCMC algorithm employed by BSR (Antonelli et al., 2020).

BSR uses an MCMC algorithm to obtain posterior distributions of \(\sigma^2\) and \(\tau_h\). In particular, Gibbs samplers are employed to sample \(\sigma^2\) and \(\tau_h\) from their full distributions and to update \(\boldsymbol\zeta\) and \(\boldsymbol\beta_S^{(h)}\). Every \(T\) MCMC iterations, BSR uses a Monte Carlo expectation maximization algorithm with a Gibbs sampler to update \(\sigma^2_{\boldsymbol\beta}\). The empirical Bayes estimate is obtained once \(\sigma^2_{\boldsymbol\beta}\) converges, at which point the MCMC runs conditional on this estimated variance.

Notably, this algorithm must deal with the explicit specification of interaction terms in the model. Any additive univariate effect or lower-order interaction term is, by definition, a subset of some higher-order interaction term. As the MCMC algorithm searches the model space, it might accept a move to a higher-order interaction and get stuck in a local mode when, in reality, a simpler model should be preferred. BSR handles this challenge by imposing a constraint in the MCMC algorithm: if the inclusion of a \(p\)th order interaction term is being considered, then the algorithm must also evaluate all \((p-1)\)th order models. If the truth is some lower-order model, then this strategy avoids the undesirable convergence to a local mode. When the model is complex, maintaining reversibility of updates under this strategy can be computationally challenging with a Gibbs sampler; in this case, using a Metropolis Hastings sampler is computationally faster (Antonelli et al., 2020).

\hypertarget{detecting-interactions}{%
\section{Detecting interactions}\label{detecting-interactions}}

In Section \ref{motivation}, we highlighted the challenges of analytically testing for the presence of interactions in exposure mixture studies. These challenges motivated a theoretical exploration of BKMR and BSR in Sections \ref{bkmr} and \ref{bsr}. Now, we discuss and compare the options that BKMR and BSR provide for inference on the presence of interactions. We also include discussion on theoretical advantages and disadvantages to each.

\hypertarget{bkmr-1}{%
\subsection{BKMR}\label{bkmr-1}}

Since the flexible \(h\) function in kernel machine regression allows us to forgo any assumptions about the nature of the relationship between the health outome and exposures, BKMR can potentially capture complex interactions between exposures The challenge with using BKMR to do this, however, is that there is no formal framework for conducting inference on the presence of interactions.

Currently, the most common approach to detecting interactions is through a qualitative assessment of visual diagnostic plots (Bobb, 2017a). Two- or three-way interactions can be assessed by plotting the estimated exposure-response relation for one/two exposures at various quantiles of another exposure, while setting all other exposures at fixed quantile values. For instance, if we are interested in the interaction between \(X_1\) and \(X_2\), we can plot the estimated regression line against \(X_1\) at the 0.25, 0.5, and 0.75 quantiles of \(X_2\) and vice versa. In the three-way case of \(X_1\), \(X_2\), and \(X_3\), we can plot the estimated regression surface against \(X_1\) and \(X_2\) at the 0.25, 0.5, and 0.75 quantiles of \(X_3\). If the shape of the estimation changes meaningfully, then there might be evidence of an interaction.

A slightly more formal inferential approach for two-way interactions involves using summary statistics (Bobb, 2017a, 2017b). In this case, we can calculate the difference in estimated response values for \(X_1\) at two quantiles, say, 0.25 and 0.75, of \(X_2\) and then generate a confidence interval. If we observe that the interval does not contain 0, then there is evidence of an interaction. The choice of quantiles here is important. If there is a parachute-like regression surface between the response and two exposures, the summary statistics might mask the true nature of the relationship.

Notably, if we specify hierarchical variable selection to handle multicollinearity, then only one exposure in each a priori defined group can enter the model. If there exists some true interaction between exposures in one group, then BKMR will be unable to incorporate it into the final model. Moreover, interactions between exposures in separate groups can only be identified if both are selected into the final model based on their within-group PIPs. Hence, if detecting interactions is a goal when using BKMR with hierarchical variable selection, groups should be carefully selected, and the influence of group membership should be considered in model interpretation.

\hypertarget{bsr-1}{%
\subsection{BSR}\label{bsr-1}}

Providing formal inference on the presence of interactions was one of the primary motivations for the development of BSR. BSR explicitly incorporates interaction terms in its model formulation, and the model fitting process assigns PIPs for any \(m\)-dimensional interaction from the posterior means of the \(\boldsymbol\zeta\) matrix. Such probabilities can be used as a quantifiable measure of the strength of a potential interaction. We can also compare PIPs for interactions with the PIPs for their individual components, which can be used to compare exposures' interactive effects with their marginal effects.

The visualization and summary statistics approaches available in BKMR are also possible in BSR. In particular, it is helpful to follow up the identification of a potential interaction with a visual assessment using the estimated exposure-response relationship at fixed quantiles of other predictors. The major benefit of BSR is that the PIPs serve as a quantifiable uncertainty metric for the presence of interactions.

\hypertarget{differences-between-bkmr-and-bsr}{%
\subsection{Differences between BKMR and BSR}\label{differences-between-bkmr-and-bsr}}

We have explained how the inferential framework of BSR improves upon the BKMR approach for the detection of complex interactions. We also briefly compare general features of their model formulations.

While BKMR is a fully nonparametric approach, BSR is a semiparametric approach because it makes distributional assumptions about the data (i.e., that the relationship can be adequately captured by a \(d\)-dimensional natural spline basis expansion). As BKMR uses the kernel technique, its implementation can become computationally intensive for datasets with large \(n\), as it scales with \(n^2\), while BSR is able to scale with \(n\).

BSR is highly sensitive to the choice of \(d\), the degrees of freedom. While it employs a WAIC approach to selecting the best \(d\), this parameter introduces an additional tuning step in the analysis. Both approaches can be highly sensitive to the specification of certain priors. BSR offers an empirical Bayes strategy for \(\sigma^2_{\boldsymbol\beta}\), the variance of a exposure's probability of inclusion. On the other hand, the choice of prior on the influential smoothing parameter, \(\rho\), in BKMR is up to the user.

We also note that BKMR offers a hierarchical variable selection approach to dealing with collinearity between exposures. While this is an additional decision that must be specified by the user, it offers a formal approach to dealing with collinear exposures. BSR does not explicitly account for collinearity in its model formulation, which can lead to erroneous interpretations of variable importance if unaccounted for.

\hypertarget{exposure-covariate-interactions}{%
\subsection{Exposure-covariate interactions}\label{exposure-covariate-interactions}}

\emph{add exposure-covariate interaction challenges}

\hypertarget{sims}{%
\chapter{Simulations}\label{sims}}

\hypertarget{past-simulation-studies}{%
\section{Past simulation studies}\label{past-simulation-studies}}

Here, we preface our simulation study with a brief overview of examples in the literature which compare various methods for exposure mixtures using simulations. Taylor et al. (2016) conclude that, in general for exposure mixture studies, no single method consistently outperforms others across all situations and, importantly, that a method should be chosen based on the question of interest. Thus, for each study, we highlight not only the findings, but also the data-generating scenarios and the identified question of interest.

Lazarevic et al. (2020) compare the performance of a broad range of methods for accurate variable selection of important exposures. They simulated exposure data using a multivariate copula based on real-world data and the response by specifying a regression relationship with only a subset of truly significant exposures and a normal error term. Two correlation structures were considered --- one with the original Spearman correlation matrix and one with the values halved --- as well as two signal-to-noise ratios --- one with an \(R^2\) for the true model at 10\% and one at 30\%. They found that BKMR, along with three other flexible regression methods that allow for nonlinearity, provided more accurate variable selection results compared to two machine learning methods. Moreover, they observed that, in general, low signal-to-noise ratios had a stronger impact on performance than did increasing multicollinearity.

Hoskovec et al. (2021) compare Bayesian methods, including BKMR, while considering 4 research questions: accurate estimation, selection of important exposures, exclusion of unimportant exposures, and identification of interactions. They use observed exposure and covariate data to simulate response data using regression relationships; they considered three exposure-response scenarios of varying complexity and included two-way multiplicative interaction terms. For each simulated dataset, they randomly assigned exposures to be active components of the mixture to incorporate variability in the data. Overall, they found that Bayesian methods outperformed traditional linear regressions, and that BKMR performed best when the exposure-response function takes on a complex form.

Most recently, Pesenti et al. (2023) compare BKMR, BSR, and the Bayesian Least Absolute Shrinkage and Selection Operator (LASSO) for variable selection. Data were generated using a multivariate normal with moderate and strong correlation structures specified manually by the researchers. They found that, in situations with additivity and linearity, Bayesian LASSO was appropriate. Across the other scenarios, BKMR generally performed best, while BSR selected exposures with high heterogeneity when the sample size was smaller due to the influence of the degrees of freedom, \(d\), tuning parameter. Notably, multicollinearity did not generally lead to spurious variable selection.

Finally, we briefly comment on studies by Sun et al. (2013) and Barrera-Gómez et al. (2017), whose explicit goal is to compare methods for identifying interactions. Both studies generate exposure data using the correlation structure from an existing dataset; Sun et al. (2013) uses a multivariate lognormal, while Barrera-Gómez et al. (2017) uses a multivariate normal. Both only consider two-way, multiplicative interactions. While neither of these studies consider the methods used in this thesis, they find that, in general, models that formally allow for interaction effects perform better than models that only allow for univariate additive effects.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

The goal of our simulation study is to provide guidance on the choice between BSR and BKMR for characterizing a diverse range of complex interactions between predictors. In particular, we aim to extend findings from previous simulation studies by considering a more comprehensive range of interaction types, including different effect sizes, non-multiplicative interactions, and three-way interactions. We also explore interactions between exposures and categorical covariates, a previously understudied form of interaction in exposure mixture studies.

\hypertarget{madres}{%
\subsection{MADRES data}\label{madres}}

In order to make our simulations comparable to real-world exposure mixture studies, we based our simulation data on the Maternal And Developmental Risks from Environmental and Social Stressors (MADRES) pregnancy cohort. The MADRES cohort is an ongoing, prospective pregnancy cohort of predominantly lower-income, Hispanic women in Los Angeles, California, which began in 2015 (Bastain et al., 2019). Urine samples were collected by participants at their first visit, and questionnaires were administered during their first visit, with follow-ups at the first, second, and third trimesters. See Bastain et al. (2019) for further details on study design.

Howe et al. (2020) previously examined the effect of prenatal metal mixtures of birth weight (BW) for gestational age (GA) in this cohort. They used BKMR to identify associations between metal mixtures and BW for GA, as well as BSR to conduct inference on interactions between metals. Briefly, using BKMR, they found that, of the metals in the mixture, Hg and Ni were most strongly associated with BW for GA. Moreover, BKMR results suggested that a potential interaction between Hg and Ni; however, when run through BSR, the PIP for this interaction was extremely small, despite being the highest of all two-way interactions.

Data from the study by Howe et al. (2020) were obtained from publicly available data in the Human Health Exposure Resource (HHEAR) Data Repository, which has been approved under Icahn School of Medicine at Mount Sinai IRB Protocol \#16-00947. The Digital Object Identifiers associated with the urinary trace element data and epidemiological data are 10.36043/1945\_159 and 10.36043/1945\_177, respectively. All analyses were conducted in R v4.3.2 (R Core Team, 2013).
\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/ch4_univlog} 

}

\caption{Distributions of original (a) and natural log transformed (b) concentrations of metals in MADRES cohort (n=252).}\label{fig:logtransf}
\end{figure}
We followed the approach by Howe et al. (2020) for preparing the data for analysis. This resulted in retaining 10 metals in analysis: arsenic (As), cadmium (Cd), cobalt (Co), mercury (Hg), nickel (Ni), molybdenum (Mo), lead (Pb), antimony (Sb), tin (Sn), and thallium (Tl). Howe et al. (2020) used speciated As, but this was not available in HHEAR, so we used total As. Metals were expressed in nanograms per milliliter (ng/mL) and natural log transformed to reduce right-skewness (Figure \ref{fig:logtransf}). Among the full range of covariates considered by Howe et al. (2020), we used the subset of 4 that were available in HHEAR: any smoke exposure during pregnancy, maternal prepregnancy body mass index (BMI), maternal age during firt trimester, and maternal race by ethnicity and birth place. We chose not to include study site, as there was a study site with only 1 participant. Race by ethnicity and birth place was collapsed into the following categories: non-Hispanic white, non-Hispanic black, non-Hispanic other, Hispanic born in the US, and Hispanic born outside the US. We observed 8 missing values for BMI in the data from HHEAR, which were not reported by Howe et al. (2020). We mean imputed these missing values. Distributions of covariates are shown in Figure \ref{fig:covdist}. Our final analytic dataset included 252 participants, which was 10 fewer than in Howe et al. (2020), likely due to small discrepancies in their dataset and the one made available in HHEAR.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch4_covdist} 

}

\caption{Distributions of continuous (a) and categorical (b) covariates in the MADRES cohort (n=252).}\label{fig:covdist}
\end{figure}
\hypertarget{copula}{%
\subsection{Using copulas to simulate predictor data}\label{copula}}

We simulated exposure and covariate data (hereafter referred to collectively as predictors) using a multivariate Gaussian copula fit on the 252 participants in the MADRES cohort. We used copulas as they can preserve both the correlation structure and marginal distributions from the observed data, allowing us to replicate conditions in a real-world scenario.

First, we briefly introduce copulas in the context of their use in this simulation, based on the presentation in Nelsen (2006). Copulas are joint cumulative distribution functions (CDFs) defined on the unit cube \([0,1]^n\) that capture the dependence between \(n\) uniformly distributed marginals. Sklar's theorem allows us to apply copulas to our observed data. Sklar's theorem states that, if \(H(x_1, \dots x_n)\) is a joint CDF of the marginal CDFs \(F_1(x_1), \dots, F_n(x_n)\), then there exists a copula \(C\) such that, for all \((x_1, \dots, x_n)\) in \((X_1, \dots, X_n)\),

\[
H(x_1, \dots x_n)=C(F_1(x_1), \dots, F_n(x_n)).
\]

\noindent Note that, by the probability integral transform, or the universality of the uniform, the CDFs \(F_1(x_1), \dots, F_n(x_n)\) are distributed uniformly.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch4_race_exp} 

}

\caption{Association between race by ethnicity and birth place and metal exposures in the MADRES cohort (n=252).}\label{fig:raceexp}
\end{figure}
We used the \texttt{copula} package in R to fit copulas and generate random data (Hofert, Kojadinovic, Maechler, \& Yan, 2023). We transformed the observed continuous predictor values to uniform distributions based on their empirical marginal CDFs, a process called generating ``pseudo-random'' samples. We used the checkerboard copula approach for generating pseudo-random samples for smoke exposure, a binary variable (Genest \& Nešlehovà, 2007). We coded smoke exposure as 0's and 1's, generated a pseudo-random sample, and then ``jittered'' the values with uniform random noise. There is currently no widely accepted approach for generating pseudo-random samples from unordered categorical variables with more than two levels. Thus, we excluded race by ethnicity and birthplace from the copula model. While this means that our simulated datasets did not preserve any potential association between race and exposures, Figure \ref{fig:raceexp} suggests that there is little to no visible association between race and exposures in the observed dataset.

Various families of copulas have been described, each of which specifies a different shape for the dependence structure. We performed model selection to identify the copula that best approximates the dependence structure of our data. We fit the set of multivariate copulas used by Lazarevic et al. (2020) in their simulation study, which included the Gaussian, \(t\), Gumbel, Frank, Clayton, and Joe copulas. We fit two \(t\) copulas with 4 and 10 degrees of freedom, which controls dependence at the tails of the distributions, as well as a \(t\) copula where the degrees of freedom was determined during the fitting process. The Gumbel, Frank, Clayton, and Joe copulas require a \(\theta\) parameter, which controls dependence between the distributions. We fit two versions of these copulas with \(\theta=\{2, 4\}\). Among these, the Gaussian copula minimized Akaike information criterion and maximized likelihood, so we proceeded with this model. The Gaussian copula assumes a bivariate normal dependence structure between the marginal CDFs.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch4_univ_exp_sim} 

}

\caption{Distributions of exposures from observed data (blue) and simulated data (gray).}\label{fig:univexpsim}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/ch4_univ_cov_sim} 

}

\caption{Distributions of covariates from observed data (blue) and simulated data (gray).}\label{fig:univcovsim}
\end{figure}
We simulated predictor data by randomly sampling from the fitted multivariate Gaussian copula distribution. All pseudo-random samples were then back-transformed to their original distributions using empirical marginal CDFs. We simulated the race by ethnicity and birthplace variable by randomly assigning observations to each of the five categories based on proportions in the observed dataset. We verified that the original structure of the observed dataset were preserved by visually comparing univariate distributions of exposures (Figure \ref{fig:univexpsim}) and covariates (Figure @(ref:univcovsim)), as well as the correlation structure using Spearman's \(\rho\) (Figure \ref{fig:corsims}).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/ch4_corr_simorigdens} 

}

\caption{Correlation heat maps of exposures from observed data (a) and averaged from simulated data (b), as well as distribution of correlations from simulated data (c).}\label{fig:corsims}
\end{figure}
We generated one set of simulated datasets with the same sample size as the observed dataset (n=252), which is typical in many cohort studies. We also generated another set of simulated datasets with a larger sample size (n=1000), which has become increasingly common with the rise of larger-scale studies and big data. The goal of this choice is to inform sample size considerations in study design.

\emph{include similar plots for larger dataset?}

\hypertarget{simresp}{%
\subsection{Simulating predictor-response relationships}\label{simresp}}

Health outcome responses were simulated under several different scenarios, each of which included different effect sizes and functional forms for the interactions, as well as different sample sizes. In the first scenario, we specified a ``base case'' model:
\begin{multline*}
Y_i = \textrm{Hg}_i + \frac{3}{1+\textrm{exp}(-4\textrm{Ni}_i)} + \frac{1.5}{1+\textrm{exp}(-4\textrm{Sn}_i)} - \textrm{Sb}_i^2 + 0.5\textrm{Sb}_i\\
+ \textrm{age} + 0.5\textrm{bmi} + 0.5\textrm{race}_{\textrm{black}} + 0.5\textrm{race}_{\textrm{hisp.non}} + 1.5\textrm{smoke} + \varepsilon_i,
\end{multline*}
\noindent where \(\varepsilon_i \overset{\mathrm{iid}}{\sim} N(0,5)\). This model includes a linear term for Hg, two S-shaped logistic terms for Ni and Sn with varying effect sizes, and a symmetric inverse U-shaped quadratic term for Sb. Moreover, we included covariate terms as linear effects in the model. We chose the standard deviation on the normal random error term in order to achieve an \(R^2\) of around 0.2-0.3 in a multiple linear regression including only the true functional form of the chemicals, which approximates realistic signal-to-noise ratios in exposure mixture studies (Lazarevic et al. (2020)).

In subsequent scenarios, we added an additional interaction term to the base case model.

The larger effect sizes were selected in order to achieve a power of approximately 0.5 at \(\alpha=0.05\) in a multiple linear regression with the true functional form of the chemicals specified, as well as with the covariate terms included.
\begin{table}

\caption{\label{tab:unnamed-chunk-2}Specification of interaction terms in simulations.}
\centering
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{10em}cc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Effect size} \\
\cmidrule(l{3pt}r{3pt}){2-3}
 & Small & Large\\
\midrule
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Univariately significant}}\\
\hspace{1em}Multiplicative & 0.35Hg$*$Ni & 0.7Hg$*$Ni\\
\hspace{1em}Polynomial & 0.3Hg$*($Ni$-1)^2$ & 0.26Hg$*($Ni$-1)^2$\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Univariately insignificant}}\\
\hspace{1em}Multiplicative & 0.35Cd$*$As & 0.7Cd$*$As\\
\hspace{1em}Polynomial & 0.125Cd$*($As$-1)^2$ & 0.25Cd$*($As$-1)^2$\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Highly correlated}}\\
\hspace{1em}Multiplicative & 0.3Hg$*$Co & 0.6Hg$*$Co\\
\hspace{1em}Polynomial & 0.09Hg$*($Co$-1)^2$ & 0.2Hg$*($Co$-1)^2$\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Three-way interaction}}\\
\hspace{1em}Multiplicative & 0.3Hg$*$Ni$*$Tl & 0.6Hg$*$Ni$*$Tl\\
\hspace{1em}Polynomial & 0.1Hg$*($Ni$-1)^2*$Tl & 0.18Hg$*($Ni$-1)^2*$Tl\\
\bottomrule
\end{tabular}
\end{table}
All cases:
\begin{itemize}
\item
  252 and 1000 sample sizes
\item
  multiplicative and polynomial interaction
\item
  small and large effect size
\item
  two-way interactions between univariately significant, univariately insignificant, and highly correlated chemicals
\item
  three-way interaction
\item
  interaction between covariate and exposure
\item
  see appendix for surfaces
\end{itemize}
This resulted in a total of 42 scenarios. For each scenario, we generated 100 simulated datasets, resulting in 4200 datasets.

\hypertarget{models}{%
\subsection{Models}\label{models}}

Software: Bobb et al. (2018) on CRAN, Antonelli et al. (2020) on GitHub

Models compared, specify the parameters for each (justify them!).

All metal concentrations were standardized to keep values scale-free.
\begin{itemize}
\tightlist
\item
  MLR
\item
  MLR with known form of interactions specified (oracle method)
\item
  BKMR with component-wise
\item
  BSR
\end{itemize}
BKMR:
Howe ran 200,000 Markov chain Monte Carlo (MCMC) iterations using the default priors. The first half of iterations was used as burn-in. To reduce potential autocorrelation, we thinned the chains, selecting every 25th iteration.
- bobb recommends 50,000 iterations at least

check convergence with trace plots

\hypertarget{model-assessment}{%
\subsection{Model assessment}\label{model-assessment}}
\begin{itemize}
\tightlist
\item
  use median probability model threshold --- marginal PIP of at least 0.5
\item
  how many times is interaction picked up?
  \begin{itemize}
  \tightlist
  \item
    sensitivity and false discovery rate
  \end{itemize}
\item
  potentially explore mpower package
\end{itemize}
\hypertarget{results}{%
\section{Results}\label{results}}
\begin{itemize}
\tightlist
\item
  example output from representative model
\item
  figures + tables w/ model performance
\end{itemize}
\hypertarget{conclusion}{%
\chapter*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{chapter}{Conclusion}

If we don't want the conclusion to have a chapter number next to it, we can add the \texttt{\{-\}} attribute.

\textbf{More info}

And here's some other random info: the first paragraph after a chapter title or section head \emph{shouldn't be} indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there.

\appendix

\hypertarget{supplemental-output}{%
\chapter{Supplemental output}\label{supplemental-output}}

\hypertarget{suppmethods}{%
\section{Methods}\label{suppmethods}}

3d surfaces of interactions.
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/p00} 

}

\caption{Surface for base case.}\label{fig:basesurf}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/am1} 

}

\caption{Surface for Hg, Ni, mult low eff.}\label{fig:am1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/am2} 

}

\caption{Surface for Hg, Ni, mult high eff.}\label{fig:am2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/ap1} 

}

\caption{Surface for Hg, Ni, poly low eff.}\label{fig:ap1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/ap2} 

}

\caption{Surface for Hg, Ni, poly high eff.}\label{fig:ap2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/bm1} 

}

\caption{Surface for Cd, As, mult low eff.}\label{fig:bm1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/bm2} 

}

\caption{Surface for Cd, As, mult high eff.}\label{fig:bm2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/bp1} 

}

\caption{Surface for Cd, As, poly low eff.}\label{fig:bp1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/bp2} 

}

\caption{Surface for Cd, As, poly high eff.}\label{fig:bp2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/cm1} 

}

\caption{Surface for Ni, Co, mult low eff.}\label{fig:cm1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/cm2} 

}

\caption{Surface for Ni, Co, mult high eff.}\label{fig:cm2}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/cp1} 

}

\caption{Surface for Ni, Co, poly low eff.}\label{fig:cp1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/surfaces/cp2} 

}

\caption{Surface for Ni, Co, poly low eff.}\label{fig:cp2}
\end{figure}
\hypertarget{results-1}{%
\section{Results}\label{results-1}}

Examples of trace plots for BKMR and BSR

\hypertarget{code}{%
\chapter{Code}\label{code}}

This second appendix includes all of the R chunks of code that were hidden throughout the document.

\hypertarget{code-for-chapter-refbayes}{%
\section{Code for Chapter \ref{bayes}:}\label{code-for-chapter-refbayes}}

The code for this chapter generates a toy example, used to demonstrate the kernel machine regression and spline regression techniques.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(stats)}
\FunctionTok{library}\NormalTok{(splines)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set theme for plots}
\FunctionTok{theme\_set}\NormalTok{(}\FunctionTok{theme\_light}\NormalTok{())}
\FunctionTok{theme\_update}\NormalTok{(}\AttributeTok{panel.grid.major =} \FunctionTok{element\_blank}\NormalTok{(), }
             \AttributeTok{panel.grid.minor =} \FunctionTok{element\_blank}\NormalTok{())}
\FunctionTok{theme\_update}\NormalTok{(}
  \AttributeTok{strip.background =} \FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{color=}\StringTok{"gray"}\NormalTok{, }\AttributeTok{fill=}\StringTok{"white"}\NormalTok{), }
  \AttributeTok{strip.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{color =} \StringTok{"gray30"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#}
\CommentTok{\# generate simulated points}
\DocumentationTok{\#\#\#\#\#\#\#\#}

\CommentTok{\# generate data from distribution}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{) }\CommentTok{\# reproducibility}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{51}\NormalTok{)}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{51}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(x, Y)}

\CommentTok{\# plot data and linear regression line}
\NormalTok{q1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(x, Y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{exp}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
                \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =} \StringTok{"y\textasciitilde{}x"}\NormalTok{, }
              \AttributeTok{color =} \StringTok{"deepskyblue3"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"gray70"}\NormalTok{, }
              \AttributeTok{linewidth =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{se =}\NormalTok{ F)}

\CommentTok{\# save plot}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"index/figures/ch3\_toy1.png"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ q1, }\AttributeTok{device =} \StringTok{"png"}\NormalTok{, }
       \AttributeTok{width =} \DecValTok{5}\NormalTok{, }\AttributeTok{height =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#}
\CommentTok{\# kernel regression}
\DocumentationTok{\#\#\#\#\#\#\#\#}

\CommentTok{\# get normal distribution of weights around query points}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Weight }\OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{mean =} \FloatTok{12.5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# plot points colored by their weights}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(x, Y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ Weight)) }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{exp}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
                \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{12.5}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}

\CommentTok{\# plot a curve of weights}
\NormalTok{normcurv }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{250}\NormalTok{)) }
\NormalTok{normcurv}\SpecialCharTok{$}\NormalTok{Weight }\OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(normcurv}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{mean =} \FloatTok{12.5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(normcurv, }\FunctionTok{aes}\NormalTok{(x, Weight, }\AttributeTok{color =}\NormalTok{ Weight)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.4}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }

\CommentTok{\# stitch plots together}
\NormalTok{q2 }\OtherTok{\textless{}{-}}\NormalTok{ cowplot}\SpecialCharTok{::}\FunctionTok{plot\_grid}\NormalTok{(p1, p2, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{, }\AttributeTok{rel\_heights =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.3}\NormalTok{))}
\NormalTok{q2}

\CommentTok{\# save plot}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"index/figures/ch3\_toy2.png"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ q2, }\AttributeTok{device =} \StringTok{"png"}\NormalTok{, }
       \AttributeTok{width =} \DecValTok{5}\NormalTok{, }\AttributeTok{height =} \DecValTok{4}\NormalTok{)}

\CommentTok{\# fit kernel regression with sigma = 1, bandwidth = 8/3}
\NormalTok{kmr\_toy }\OtherTok{\textless{}{-}} \FunctionTok{ksmooth}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{x, df}\SpecialCharTok{$}\NormalTok{Y, }\AttributeTok{kernel =} \StringTok{"normal"}\NormalTok{, }
                   \AttributeTok{bandwidth =} \DecValTok{8}\SpecialCharTok{/}\DecValTok{3}\NormalTok{, }\AttributeTok{x.points =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{x)}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{left\_join}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(kmr\_toy), }\AttributeTok{by =} \StringTok{"x"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{Yhat =}\NormalTok{ y)}

\CommentTok{\# plot kernel regression estimation}
\NormalTok{q3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{exp}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
                \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Yhat), }\AttributeTok{color =} \StringTok{"deepskyblue3"}\NormalTok{) }
\NormalTok{q3}

\CommentTok{\# save plot}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"index/figures/ch3\_toy3.png"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ q3, }\AttributeTok{device =} \StringTok{"png"}\NormalTok{, }
       \AttributeTok{width =} \DecValTok{5}\NormalTok{, }\AttributeTok{height =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# fit kernel regression with sigma = 5, bandwidth = 40/3}
\NormalTok{kmr\_toy\_5 }\OtherTok{\textless{}{-}} \FunctionTok{ksmooth}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{x, df}\SpecialCharTok{$}\NormalTok{Y, }\AttributeTok{kernel =} \StringTok{"normal"}\NormalTok{, }
                     \AttributeTok{bandwidth =} \DecValTok{40}\SpecialCharTok{/}\DecValTok{3}\NormalTok{, }\AttributeTok{x.points =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{x)}

\CommentTok{\# fit kernel regression with sigma = 0.1, bandwith = 8/30}
\NormalTok{kmr\_toy\_1 }\OtherTok{\textless{}{-}} \FunctionTok{ksmooth}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{x, df}\SpecialCharTok{$}\NormalTok{Y, }\AttributeTok{kernel =} \StringTok{"normal"}\NormalTok{, }
                     \AttributeTok{bandwidth =} \DecValTok{8}\SpecialCharTok{/}\DecValTok{30}\NormalTok{, }\AttributeTok{x.points =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{x)}

\CommentTok{\# re{-}join data}
\NormalTok{dfrho }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{left\_join}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(kmr\_toy\_5), }\AttributeTok{by =} \StringTok{"x"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\StringTok{"rho = 50"} \OtherTok{=}\NormalTok{ y) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{left\_join}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(kmr\_toy\_1), }\AttributeTok{by =} \StringTok{"x"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\StringTok{"rho = 0.02"} \OtherTok{=}\NormalTok{ y) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Yhat) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"rho = 50"}\NormalTok{, }\StringTok{"rho = 0.02"}\NormalTok{), }\AttributeTok{values\_to =} \StringTok{"Yhat"}\NormalTok{)}

\CommentTok{\# plot kernel regression with two values of rho}
\NormalTok{qrho }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(dfrho) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Yhat), }\AttributeTok{color =} \StringTok{"deepskyblue3"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{name)}
\NormalTok{qrho}

\CommentTok{\# save plot}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"index/figures/ch3\_toyrho.png"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ qrho, }\AttributeTok{device =} \StringTok{"png"}\NormalTok{, }
       \AttributeTok{width =} \DecValTok{7}\NormalTok{, }\AttributeTok{height =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#}
\CommentTok{\# spline regression}
\DocumentationTok{\#\#\#\#\#\#\#\#}

\NormalTok{kn }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{) }\CommentTok{\# 4 knots of equal width}

\CommentTok{\# fit linear spline regression}
\NormalTok{spline\_toy\_line }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{bs}\NormalTok{(x, }\AttributeTok{knots =}\NormalTok{ kn, }\AttributeTok{degree =} \DecValTok{1}\NormalTok{), }\AttributeTok{data =}\NormalTok{ df)}
\NormalTok{p\_line }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(spline\_toy\_line, }\AttributeTok{se =}\NormalTok{ T)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Yhats\_line }\OtherTok{\textless{}{-}}\NormalTok{ p\_line}\SpecialCharTok{$}\NormalTok{fit}

\NormalTok{q4 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{exp}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
                \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Yhats\_line), }\AttributeTok{color =} \StringTok{"deepskyblue3"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ kn, }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{)}
\NormalTok{q4}

\CommentTok{\# save plot}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"index/figures/ch3\_toy4.png"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ q4, }\AttributeTok{device =} \StringTok{"png"}\NormalTok{, }
       \AttributeTok{width =} \DecValTok{5}\NormalTok{, }\AttributeTok{height =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# fit cubic spline regression}
\NormalTok{spline\_toy\_cub }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{bs}\NormalTok{(x, }\AttributeTok{knots =}\NormalTok{ kn), }\AttributeTok{data =}\NormalTok{ df)}
\NormalTok{p\_cub }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(spline\_toy\_cub, }\AttributeTok{se =}\NormalTok{ T)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Yhats\_cub }\OtherTok{\textless{}{-}}\NormalTok{ p\_cub}\SpecialCharTok{$}\NormalTok{fit}

\CommentTok{\# plot spline regression estimation}
\NormalTok{q5 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{exp}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
                \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Yhats\_cub), }\AttributeTok{color =} \StringTok{"deepskyblue3"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ kn, }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{)}
\NormalTok{q5}

\CommentTok{\# save plot}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"index/figures/ch3\_toy5.png"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ q5, }\AttributeTok{device =} \StringTok{"png"}\NormalTok{, }
       \AttributeTok{width =} \DecValTok{5}\NormalTok{, }\AttributeTok{height =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# fit natural spline regression}
\NormalTok{spline\_toy\_nat }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ns}\NormalTok{(x, }\AttributeTok{knots =}\NormalTok{ kn), }\AttributeTok{data =}\NormalTok{ df)}
\NormalTok{p\_nat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(spline\_toy\_nat, }\AttributeTok{se =}\NormalTok{ T)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Yhats\_nat }\OtherTok{\textless{}{-}}\NormalTok{ p\_nat}\SpecialCharTok{$}\NormalTok{fit}

\CommentTok{\# plot spline regression estimation}
\NormalTok{q6 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{exp}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
                \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Yhats\_nat), }\AttributeTok{color =} \StringTok{"deepskyblue3"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{)}
\NormalTok{q6}

\CommentTok{\# save plot}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"index/figures/ch3\_toy6.png"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ q6, }\AttributeTok{device =} \StringTok{"png"}\NormalTok{, }
       \AttributeTok{width =} \DecValTok{5}\NormalTok{, }\AttributeTok{height =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# see what happens outside of the bounds}
\NormalTok{x\_longer }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{30}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{81}\NormalTok{)}
\NormalTok{y\_longer\_cub }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(spline\_toy\_cub, }
                        \AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x\_longer))}
\NormalTok{y\_longer\_nat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(spline\_toy\_nat, }
                        \AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x\_longer))}

\NormalTok{df\_longer }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(x\_longer, x\_longer), }
  \AttributeTok{spline =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Cubic"}\NormalTok{, }\DecValTok{81}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Natural"}\NormalTok{, }\DecValTok{81}\NormalTok{)), }
  \AttributeTok{Yhat =} \FunctionTok{c}\NormalTok{(y\_longer\_cub, y\_longer\_nat)}
\NormalTok{)}

\CommentTok{\# plot outside of bounds}
\NormalTok{qbounds }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df\_longer) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, Yhat), }\AttributeTok{color =} \StringTok{"deepskyblue3"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_function}\NormalTok{(}\AttributeTok{fun =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{exp}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{sin}\NormalTok{(x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }
                \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{), }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{spline) }
\NormalTok{qbounds}

\CommentTok{\# save plot}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"index/figures/ch3\_toybounds.png"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ qbounds, }
       \AttributeTok{device =} \StringTok{"png"}\NormalTok{, }\AttributeTok{width =} \DecValTok{7}\NormalTok{, }\AttributeTok{height =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\hypertarget{code-for-chapter-refsims}{%
\section{Code for Chapter \ref{sims}:}\label{code-for-chapter-refsims}}

The code for this chapter prepares the data from the MADRES study, generates simulated data, fits multiple linear regressions, BKMR, and BSR on the simulated data, and produces model output.

\hypertarget{code-for-chapter-refmadres}{%
\subsection{Code for Chapter \ref{madres}:}\label{code-for-chapter-refmadres}}

First, we clean the data from the MADRES study.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in data}
\NormalTok{target }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"madres\_data/1945\_TARGETED\_DATA.csv"}\NormalTok{)}
\NormalTok{epi }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"madres\_data/1945\_EPI\_DATA.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#}
\CommentTok{\# clean target data}
\DocumentationTok{\#\#\#\#\#\#\#\#}

\NormalTok{target\_small }\OtherTok{\textless{}{-}}\NormalTok{ target }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# if below LOD, use LOD / sqrt(2)}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{conc\_mod =} \FunctionTok{ifelse}\NormalTok{(Comment\_code }\SpecialCharTok{==} \DecValTok{37}\NormalTok{, }
\NormalTok{                           LOD }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{), }
\NormalTok{                           Concentration)) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# adjust for urine specific gravity: Ac = A × [(SGmean –1)/(SG–1)]}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{conc\_mod =}\NormalTok{ conc\_mod }\SpecialCharTok{*}\NormalTok{ ((}\FunctionTok{mean}\NormalTok{(target}\SpecialCharTok{$}\NormalTok{SG)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(SG}\DecValTok{{-}1}\NormalTok{))) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(Project\_ID, SID, PID, child\_PID, Analyte\_Code, conc\_mod) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(SID) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Project\_ID =} \FunctionTok{min}\NormalTok{(Project\_ID)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Analyte\_Code, }\AttributeTok{values\_from =}\NormalTok{ conc\_mod) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# howe kept As, Cd, Co, Hg, Ni, Tl, and Pb in main, Mo, Sb, and Sn in supp}
  \CommentTok{\# don\textquotesingle{}t have modified version of As used in their paper}
  \FunctionTok{select}\NormalTok{(Project\_ID, SID, PID, child\_PID, }
\NormalTok{         As, Cd, Co, Hg, Ni, Tl, Pb, Mo, Sb, Sn)}

\CommentTok{\# save}
\FunctionTok{write\_csv}\NormalTok{(target\_small, }\StringTok{"madres\_data/target\_small.csv"}\NormalTok{)}

\CommentTok{\# only keep data from first trimester}
\NormalTok{target\_first }\OtherTok{\textless{}{-}}\NormalTok{ target\_small }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(child\_PID) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Project\_ID }\SpecialCharTok{==} \FunctionTok{min}\NormalTok{(Project\_ID)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# save}
\FunctionTok{write\_csv}\NormalTok{(target\_first, }\StringTok{"madres\_data/target\_first.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#}
\CommentTok{\# clean epi data}
\DocumentationTok{\#\#\#\#\#\#\#\#}

\CommentTok{\# select relevant variables}
\NormalTok{epi\_small }\OtherTok{\textless{}{-}}\NormalTok{ epi }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# make new categorical variables}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mom\_site =} \FunctionTok{as.factor}\NormalTok{(mom\_site), }
         \AttributeTok{race =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{case\_when}\NormalTok{(}
\NormalTok{           t1\_demo\_hispanic }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ t1\_demo\_race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\CommentTok{\#non{-}hisp white}
\NormalTok{           t1\_demo\_hispanic }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ t1\_demo\_race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{, }\CommentTok{\#non{-}hisp black}
\NormalTok{           t1\_demo\_hispanic }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{, }\CommentTok{\#other, non{-}hispanic}
\NormalTok{           t1\_demo\_hispanic }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ t1\_demo\_usa }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{4}\NormalTok{, }\CommentTok{\#hispanic in US}
\NormalTok{           t1\_demo\_hispanic }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ t1\_demo\_usa }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \DecValTok{5}\NormalTok{, }\CommentTok{\#hispanic NOT in US}
           \AttributeTok{.default =} \ConstantTok{NA}
\NormalTok{         )), }
         \AttributeTok{smoke =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}
\NormalTok{           t1\_smoke\_preg }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ t2\_smoke\_preg }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ t3\_smoke\_preg }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{|}
\NormalTok{             t1\_smoke }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ t2\_smoke }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ t3\_smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}
\NormalTok{         ))) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# replace {-}99 with NA}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{ifelse}\NormalTok{(. }\SpecialCharTok{==} \SpecialCharTok{{-}}\DecValTok{99}\NormalTok{, }\ConstantTok{NA}\NormalTok{, .)))  }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(child\_pid, mom\_site, }
                \AttributeTok{age =}\NormalTok{ t1\_mat\_age, }\CommentTok{\# age, trimester 1}
                \AttributeTok{bmi =}\NormalTok{ t1\_pre\_BMI, }\CommentTok{\# bmi}
\NormalTok{                race, }\CommentTok{\# maternal r/e}
\NormalTok{                smoke, }\CommentTok{\# ever{-}exposure to smoke}
\NormalTok{                gender, birthweight, GA }\CommentTok{\# birthweight + gestational age}
                \CommentTok{\# can\textquotesingle{}t find anemia measure or AsB}
\NormalTok{  ) }

\CommentTok{\# handle NA values}
\NormalTok{epi\_imp }\OtherTok{\textless{}{-}}\NormalTok{ epi\_small }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# exclude birthweight (observed response)}
  \CommentTok{\# exclude study site because of small categories}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(gender, birthweight, GA, mom\_site)) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# na\textquotesingle{}s for smoke during preg, set to 0}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{smoke =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(smoke), }\DecValTok{0}\NormalTok{, smoke))) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# impute mean for BMI}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric), }
                \SpecialCharTok{\textasciitilde{}}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.), }\FunctionTok{mean}\NormalTok{(.,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), .))) }
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#}
\CommentTok{\# combine epi and target data}
\DocumentationTok{\#\#\#\#\#\#\#\#}
\NormalTok{comb }\OtherTok{\textless{}{-}}\NormalTok{ epi\_imp }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{left\_join}\NormalTok{(target\_first, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"child\_pid"} \OtherTok{=} \StringTok{"child\_PID"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{relocate}\NormalTok{(child\_pid, Project\_ID, SID, PID, mom\_site, race, smoke) }

\CommentTok{\# remove outliers}
\NormalTok{comb\_small }\OtherTok{\textless{}{-}}\NormalTok{ comb }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Mo }\SpecialCharTok{\textgreater{}=}\DecValTok{1}\NormalTok{, Sb }\SpecialCharTok{\textless{}=} \FloatTok{1.4}\NormalTok{)}

\CommentTok{\# save}
\FunctionTok{write\_csv}\NormalTok{(comb\_small, }\StringTok{"madres\_data/base\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\hypertarget{code-for-chapter-refcopula}{%
\subsection{Code for Chapter \ref{copula}:}\label{code-for-chapter-refcopula}}

Next, we use copulas to simulate predictor data. We use the \texttt{copula} and \texttt{rslurm} packages in this section. This code was run on the Amherst HPC RStudio server.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(copula)}
\FunctionTok{library}\NormalTok{(rslurm)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read data back in}
\NormalTok{comb\_small }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"madres\_data/base\_data.csv"}\NormalTok{)}

\CommentTok{\# log{-}transform target data}
\NormalTok{comb\_log }\OtherTok{\textless{}{-}}\NormalTok{ comb\_small }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\DecValTok{10}\SpecialCharTok{:}\DecValTok{19}\NormalTok{, log)) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# factors back to numeric}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.factor), as.numeric))}

\CommentTok{\# check spearman\textquotesingle{}s rho}
\FunctionTok{cor}\NormalTok{(comb\_log[, }\DecValTok{7}\SpecialCharTok{:}\DecValTok{19}\NormalTok{], }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#}
\CommentTok{\# fit copulas}
\DocumentationTok{\#\#\#\#\#\#\#\#}

\CommentTok{\# create pseudo observations for continuous variables}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{pobs}\NormalTok{(comb\_log[, }\DecValTok{7}\SpecialCharTok{:}\DecValTok{19}\NormalTok{])}

\CommentTok{\# fit checkerboard copula on smoke}
\NormalTok{prop\_smoke0 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(comb\_log}\SpecialCharTok{$}\NormalTok{smoke)}
\CommentTok{\# jitter 0\textquotesingle{}s and 1\textquotesingle{}s uniformly within quantile}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{u\_smoke }\OtherTok{\textless{}{-}}\NormalTok{ comb\_log}\SpecialCharTok{$}\NormalTok{smoke }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{map\_dbl}\NormalTok{(\textbackslash{}(x) \{}
    \FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, prop\_smoke0), }\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, prop\_smoke0, }\DecValTok{1}\NormalTok{))}
\NormalTok{  \})}
\NormalTok{u[, }\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ u\_smoke}

\CommentTok{\# fit copulas}
\NormalTok{cfit\_gaus }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{normalCopula}\NormalTok{(}\AttributeTok{dim =} \DecValTok{13}\NormalTok{, }\AttributeTok{dispstr =} \StringTok{"un"}\NormalTok{), u)}
\NormalTok{cfit\_t1 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{tCopula}\NormalTok{(}\AttributeTok{dim =} \DecValTok{13}\NormalTok{, }\AttributeTok{dispstr =} \StringTok{"un"}\NormalTok{, }
                             \AttributeTok{df.fixed =} \ConstantTok{FALSE}\NormalTok{), u)}
\NormalTok{cfit\_t2 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{tCopula}\NormalTok{(}\AttributeTok{dim =} \DecValTok{13}\NormalTok{, }\AttributeTok{dispstr =} \StringTok{"un"}\NormalTok{, }
                             \AttributeTok{df =} \DecValTok{4}\NormalTok{, }\AttributeTok{df.fixed =} \ConstantTok{TRUE}\NormalTok{), u)}
\NormalTok{cfit\_t3 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{tCopula}\NormalTok{(}\AttributeTok{dim =} \DecValTok{13}\NormalTok{, }\AttributeTok{dispstr =} \StringTok{"un"}\NormalTok{, }
                             \AttributeTok{df =} \DecValTok{10}\NormalTok{, }\AttributeTok{df.fixed =} \ConstantTok{TRUE}\NormalTok{), u)}
\NormalTok{cfit\_gum1 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{gumbelCopula}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{dim =} \DecValTok{13}\NormalTok{), u)}
\NormalTok{cfit\_gum2 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{gumbelCopula}\NormalTok{(}\DecValTok{2}\NormalTok{, }\AttributeTok{dim =} \DecValTok{13}\NormalTok{), u)}
\NormalTok{cfit\_frank1 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{frankCopula}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{dim =} \DecValTok{13}\NormalTok{), u)}
\NormalTok{cfit\_frank2 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{frankCopula}\NormalTok{(}\DecValTok{2}\NormalTok{, }\AttributeTok{dim =} \DecValTok{13}\NormalTok{), u)}
\NormalTok{cfit\_clay1 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{claytonCopula}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{dim =} \DecValTok{13}\NormalTok{), u)}
\NormalTok{cfit\_clay2 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{claytonCopula}\NormalTok{(}\DecValTok{2}\NormalTok{, }\AttributeTok{dim =} \DecValTok{13}\NormalTok{), u)}
\NormalTok{cfit\_joe1 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{joeCopula}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{dim =} \DecValTok{13}\NormalTok{), u)}
\NormalTok{cfit\_joe2 }\OtherTok{\textless{}{-}} \FunctionTok{fitCopula}\NormalTok{(}\FunctionTok{joeCopula}\NormalTok{(}\DecValTok{2}\NormalTok{, }\AttributeTok{dim =} \DecValTok{13}\NormalTok{), u)}

\CommentTok{\# evaluate fit using AIC}
\NormalTok{aic\_values }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{list}\NormalTok{(cfit\_gaus, cfit\_t1, cfit\_t2, cfit\_t3, }
\NormalTok{                          cfit\_gum1, cfit\_gum2, cfit\_frank1, cfit\_frank2, }
\NormalTok{                          cfit\_clay1, cfit\_clay2, cfit\_joe1, cfit\_joe2}
\NormalTok{                          ), AIC)}
\FunctionTok{names}\NormalTok{(aic\_values) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cfit\_gaus"}\NormalTok{, }\StringTok{"cfit\_t1"}\NormalTok{, }\StringTok{"cfit\_t2"}\NormalTok{, }\StringTok{"cfit\_t3"}\NormalTok{,}
                       \StringTok{"cfit\_gum"}\NormalTok{, }\StringTok{"cfit\_gum2"}\NormalTok{, }\StringTok{"cfit\_frank1"}\NormalTok{, }\StringTok{"cfit\_frank2"}\NormalTok{, }
                       \StringTok{"cfit\_clay1"}\NormalTok{, }\StringTok{"cfit\_clay2"}\NormalTok{, }\StringTok{"cfit\_joe1"}\NormalTok{, }\StringTok{"cfit\_joe2"}\NormalTok{)}
\FunctionTok{sort}\NormalTok{(aic\_values)}

\CommentTok{\# evaluate fit using likelihood}
\NormalTok{aic\_values }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{list}\NormalTok{(cfit\_gaus, cfit\_t1, cfit\_t2, cfit\_t3, }
\NormalTok{                          cfit\_gum1, cfit\_gum2, cfit\_frank1, cfit\_frank2, }
\NormalTok{                          cfit\_clay1, cfit\_clay2, cfit\_joe1, cfit\_joe2}
\NormalTok{                          ), logLik)}
\FunctionTok{names}\NormalTok{(lik\_values) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cfit\_gaus"}\NormalTok{, }\StringTok{"cfit\_t1"}\NormalTok{, }\StringTok{"cfit\_t2"}\NormalTok{, }\StringTok{"cfit\_t3"}\NormalTok{,}
                       \StringTok{"cfit\_gum"}\NormalTok{, }\StringTok{"cfit\_gum2"}\NormalTok{, }\StringTok{"cfit\_frank1"}\NormalTok{, }\StringTok{"cfit\_frank2"}\NormalTok{, }
                       \StringTok{"cfit\_clay1"}\NormalTok{, }\StringTok{"cfit\_clay2"}\NormalTok{, }\StringTok{"cfit\_joe1"}\NormalTok{, }\StringTok{"cfit\_joe2"}\NormalTok{)}
\FunctionTok{sort}\NormalTok{(lik\_values)}

\CommentTok{\# guassian copula performs best, proceed with this}
\FunctionTok{write\_rds}\NormalTok{(cfit\_gaus, }\StringTok{"sim/gauscop.RDS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#}
\CommentTok{\# simulate predictor data}
\DocumentationTok{\#\#\#\#\#\#\#\#}

\CommentTok{\# read copula back in }
\NormalTok{cfit\_gaus }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{(}\StringTok{"sim/gauscop.RDS"}\NormalTok{)}

\CommentTok{\# extract rho }
\NormalTok{rho }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(cfit\_gaus)}

\CommentTok{\# create function for simulation}
\NormalTok{simulate\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, n, rho, prop\_smoke, prop\_race) \{}
  \CommentTok{\#\textquotesingle{} data = original observed data}
  \CommentTok{\#\textquotesingle{} n = sample size}
  \CommentTok{\#\textquotesingle{} rho = rho values from normal copula}
  \CommentTok{\#\textquotesingle{} prop\_smoke = proportion smoke from observed dataset}
  \CommentTok{\#\textquotesingle{} prop\_race = table with race/eth values}
  
  \CommentTok{\# simulate pseudo{-}observations from copula}
\NormalTok{  samp }\OtherTok{\textless{}{-}} \FunctionTok{rCopula}\NormalTok{(n, }\FunctionTok{normalCopula}\NormalTok{(rho, }\AttributeTok{dim =} \FunctionTok{ncol}\NormalTok{(data), }\AttributeTok{dispstr =} \StringTok{"un"}\NormalTok{))}
  
  \CommentTok{\# transform pseudo{-}observations to observed marginal distributions}
\NormalTok{  sampt }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(data) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map\_dfc}\NormalTok{(}
\NormalTok{      \textbackslash{}(x) \{}
        \ControlFlowTok{if}\NormalTok{(}\FunctionTok{names}\NormalTok{(data)[x] }\SpecialCharTok{==} \StringTok{"smoke"}\NormalTok{) \{}
          \CommentTok{\# use observed probability threshold for smoke}
\NormalTok{          df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(samp[,x] }\SpecialCharTok{\textless{}}\NormalTok{ prop\_smoke, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                           \AttributeTok{row.names =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
          \CommentTok{\# use empirical marginal CDF\textquotesingle{}s for continuous}
\NormalTok{          df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(data[[x]], }\AttributeTok{probs =}\NormalTok{ samp[,x]), }
                           \AttributeTok{row.names =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{        \}}
        \FunctionTok{names}\NormalTok{(df) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(data)[x]}
        \FunctionTok{return}\NormalTok{(df)}
\NormalTok{      \}}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}} 
    \CommentTok{\# randomly sample race}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{race =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{names}\NormalTok{(prop\_race), }\AttributeTok{prob =}\NormalTok{ prop\_race,}
                         \AttributeTok{size =}\NormalTok{ n, }\AttributeTok{replace =}\NormalTok{ T)) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{relocate}\NormalTok{(race)}
  \FunctionTok{return}\NormalTok{(sampt)}
\NormalTok{\}}

\CommentTok{\# create function to run size 252 samples on hpc}
\NormalTok{run\_sim1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{  out }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2100} \SpecialCharTok{|\textgreater{}} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(\textbackslash{}(x) \{}
      \FunctionTok{mutate}\NormalTok{(}\FunctionTok{simulate\_data}\NormalTok{(comb\_log\_clip, }\AttributeTok{n =} \FunctionTok{nrow}\NormalTok{(comb\_log\_clip), }\AttributeTok{rho =}\NormalTok{ rho, }
                           \AttributeTok{prop\_smoke =} \DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(comb\_log\_clip}\SpecialCharTok{$}\NormalTok{smoke), }
                           \AttributeTok{prop\_race =} \FunctionTok{table}\NormalTok{(comb\_log}\SpecialCharTok{$}\NormalTok{race)), }
             \AttributeTok{race =} \FunctionTok{as.numeric}\NormalTok{(race), }
             \AttributeTok{sim =}\NormalTok{ x) }
\NormalTok{    \})}
  \FunctionTok{return}\NormalTok{(out)}
\NormalTok{\}}

\CommentTok{\# send job to hpc for size 252 samples}
\NormalTok{sjob1 }\OtherTok{\textless{}{-}} \FunctionTok{slurm\_call}\NormalTok{(run\_sim1, }
                    \AttributeTok{global\_objects =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}comb\_log\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}comb\_log\_clip\textquotesingle{}}\NormalTok{, }
                                       \StringTok{\textquotesingle{}rho\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}simulate\_data\textquotesingle{}}\NormalTok{),}
                    \AttributeTok{jobname =} \StringTok{\textquotesingle{}sim\_data1\textquotesingle{}}\NormalTok{)}

\CommentTok{\# get output}
\NormalTok{out1 }\OtherTok{\textless{}{-}} \FunctionTok{get\_slurm\_out}\NormalTok{(sjob1)}
\FunctionTok{write\_rds}\NormalTok{(out1, }\StringTok{"sim/sim\_preds\_sm.RDS"}\NormalTok{)}

\CommentTok{\# create function to run size 1000 samples on hpc}
\NormalTok{run\_sim2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  out }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2100} \SpecialCharTok{|\textgreater{}} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(\textbackslash{}(x) \{}
      \FunctionTok{mutate}\NormalTok{(}\FunctionTok{simulate\_data}\NormalTok{(comb\_log\_clip, }\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{rho =}\NormalTok{ rho, }
                           \AttributeTok{prop\_smoke =} \DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(comb\_log\_clip}\SpecialCharTok{$}\NormalTok{smoke), }
                           \AttributeTok{prop\_race =} \FunctionTok{table}\NormalTok{(comb\_log}\SpecialCharTok{$}\NormalTok{race)), }
             \AttributeTok{race =} \FunctionTok{as.numeric}\NormalTok{(race), }
             \AttributeTok{sim =}\NormalTok{ x)}
\NormalTok{    \})}
  \FunctionTok{return}\NormalTok{(out)}
\NormalTok{\}}

\CommentTok{\# send job to hpc for size 1000 samples}
\NormalTok{sjob2 }\OtherTok{\textless{}{-}} \FunctionTok{slurm\_call}\NormalTok{(run\_sim2, }
                    \AttributeTok{global\_objects =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}comb\_log\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}comb\_log\_clip\textquotesingle{}}\NormalTok{, }
                                       \StringTok{\textquotesingle{}rho\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}simulate\_data\textquotesingle{}}\NormalTok{),}
                    \AttributeTok{jobname =} \StringTok{\textquotesingle{}sim\_data2\textquotesingle{}}\NormalTok{)}

\CommentTok{\# get output}
\NormalTok{out2 }\OtherTok{\textless{}{-}} \FunctionTok{get\_slurm\_out}\NormalTok{(sjob2)}
\FunctionTok{write\_rds}\NormalTok{(out2, }\StringTok{"sim/sim\_preds\_lg.RDS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\hypertarget{code-for-chapter-refsimresp}{%
\subsection{Code for Chapter \ref{simresp}:}\label{code-for-chapter-refsimresp}}

Next, we simulate the response data. We use the \texttt{rslurm} package in this section. This code was run on the Amherst HPC RStudio server.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(rslurm)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# create functions for various response variables}
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#\#\#\#}

\CommentTok{\# base case, no interactions}
\NormalTok{base\_case }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{am1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.35}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{Ni }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{am2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.7}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{Ni }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{ap1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.13}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{((Ni}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{ap2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.26}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{((Ni}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{bm1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.35}\SpecialCharTok{*}\NormalTok{Cd}\SpecialCharTok{*}\NormalTok{As }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{bm2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.7}\SpecialCharTok{*}\NormalTok{Cd}\SpecialCharTok{*}\NormalTok{As }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{bp1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.125}\SpecialCharTok{*}\NormalTok{Cd}\SpecialCharTok{*}\NormalTok{((As}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{bp2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.25}\SpecialCharTok{*}\NormalTok{Cd}\SpecialCharTok{*}\NormalTok{((As}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{cm1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.3}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{Co }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{cm2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.6}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{Co }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{cp1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.15}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{((Co}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{cp2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.3}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{((Co}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{dm1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.3}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{Ni}\SpecialCharTok{*}\NormalTok{Tl }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{dm2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.6}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{Ni}\SpecialCharTok{*}\NormalTok{Tl }\SpecialCharTok{+} 
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{dp1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.09}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{((Ni}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\NormalTok{Tl }\SpecialCharTok{+}
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}

\NormalTok{dp2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df) \{}
  \FunctionTok{mutate}\NormalTok{(df, }\AttributeTok{y =} 
\NormalTok{           Hg }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Ni)) }\SpecialCharTok{{-}}\NormalTok{ (Sb}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{Sb }\SpecialCharTok{+} \FloatTok{1.5}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1}\SpecialCharTok{+}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\SpecialCharTok{*}\NormalTok{Sn)) }\SpecialCharTok{+} 
           \FloatTok{0.18}\SpecialCharTok{*}\NormalTok{Hg}\SpecialCharTok{*}\NormalTok{((Ni}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\NormalTok{Tl }\SpecialCharTok{+}
\NormalTok{           age }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{bmi }\SpecialCharTok{+} 
           \FunctionTok{case\_when}\NormalTok{(race }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
\NormalTok{                     race }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{ifelse}\NormalTok{(smoke }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# create response variables for exposure{-}exposure interxn}
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#\#\#}

\CommentTok{\# read output back in, size 252}
\NormalTok{out1 }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{(}\StringTok{"sim/sim\_preds\_sm.RDS"}\NormalTok{)}

\CommentTok{\# create function for responses at size 252}
\NormalTok{run\_resp1 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{  out1\_resp1 }\OtherTok{\textless{}{-}}\NormalTok{ out1 }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(\textbackslash{}(x) \{}
      \CommentTok{\# get dataset number }
\NormalTok{      no }\OtherTok{\textless{}{-}}\NormalTok{ x}\SpecialCharTok{$}\NormalTok{sim[}\DecValTok{1}\NormalTok{] }
\NormalTok{      x }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{|\textgreater{}} 
        \CommentTok{\# scale log{-}transformed exposures}
        \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(As}\SpecialCharTok{:}\NormalTok{Sn, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{c}\NormalTok{(}\FunctionTok{scale}\NormalTok{(.)))) }
\NormalTok{      df }\OtherTok{\textless{}{-}} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{100} \SpecialCharTok{\textasciitilde{}} \FunctionTok{base\_case}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{200} \SpecialCharTok{\textasciitilde{}} \FunctionTok{am1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{300} \SpecialCharTok{\textasciitilde{}} \FunctionTok{am2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{400} \SpecialCharTok{\textasciitilde{}} \FunctionTok{ap1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{500} \SpecialCharTok{\textasciitilde{}} \FunctionTok{ap2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{600} \SpecialCharTok{\textasciitilde{}} \FunctionTok{bm1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{700} \SpecialCharTok{\textasciitilde{}} \FunctionTok{bm2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{800} \SpecialCharTok{\textasciitilde{}} \FunctionTok{bp1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{900} \SpecialCharTok{\textasciitilde{}} \FunctionTok{bp2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1000} \SpecialCharTok{\textasciitilde{}} \FunctionTok{cm1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1100} \SpecialCharTok{\textasciitilde{}} \FunctionTok{cm2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1200} \SpecialCharTok{\textasciitilde{}} \FunctionTok{cp1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1300} \SpecialCharTok{\textasciitilde{}} \FunctionTok{cp2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1400} \SpecialCharTok{\textasciitilde{}} \FunctionTok{dm1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1500} \SpecialCharTok{\textasciitilde{}} \FunctionTok{dm2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1600} \SpecialCharTok{\textasciitilde{}} \FunctionTok{dp1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1700} \SpecialCharTok{\textasciitilde{}} \FunctionTok{dp2}\NormalTok{(x), }
        \AttributeTok{.default =}\NormalTok{ x }\CommentTok{\#note 1701 {-} 2100 is for cov{-}exp interxn}
\NormalTok{      ) }
\NormalTok{    \}) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{set\_names}\NormalTok{(}\AttributeTok{nm =} \FunctionTok{c}\NormalTok{(}
      \FunctionTok{rep}\NormalTok{(}\StringTok{"\_base"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"am1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"am2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"ap1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"ap2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"bm1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"bm2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"bp1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"bp2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"cm1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"cm2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"cp1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"cp2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"dm1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"dm2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"dp1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"dp2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"unset"}\NormalTok{, }\DecValTok{400}\NormalTok{)}
\NormalTok{    )) }
  \FunctionTok{return}\NormalTok{(out1\_resp1)}
\NormalTok{\}}

\CommentTok{\# run to hpc}
\NormalTok{runrespsm }\OtherTok{\textless{}{-}} \FunctionTok{slurm\_call}\NormalTok{(}
\NormalTok{  run\_resp1, }
  \AttributeTok{global\_objects =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}out1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}base\_case\textquotesingle{}}\NormalTok{, }
                     \StringTok{\textquotesingle{}am1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}am2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ap1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ap2\textquotesingle{}}\NormalTok{, }
                     \StringTok{\textquotesingle{}bm1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bm2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bp1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bp2\textquotesingle{}}\NormalTok{, }
                     \StringTok{\textquotesingle{}cm1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cm2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cp1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cp2\textquotesingle{}}\NormalTok{, }
                     \StringTok{\textquotesingle{}dm1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dm2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dp1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dp2\textquotesingle{}}\NormalTok{),}
  \AttributeTok{jobname =} \StringTok{\textquotesingle{}sim\_resp1\textquotesingle{}}\NormalTok{)}

\CommentTok{\# get output}
\NormalTok{out1\_resp1 }\OtherTok{\textless{}{-}} \FunctionTok{get\_slurm\_out}\NormalTok{(runrespsm)}
\CommentTok{\# only save for exp{-}exp interxns}
\NormalTok{out1\_resp1 }\OtherTok{\textless{}{-}}\NormalTok{ out1\_resp1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1700}\NormalTok{]}
\FunctionTok{write\_rds}\NormalTok{(out1\_resp1, }\StringTok{"sim/sim\_resp\_sm\_a.RDS"}\NormalTok{)}

\CommentTok{\# read output back in, size 1000}
\NormalTok{out2 }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{(}\StringTok{"sim/sim\_preds\_lg.RDS"}\NormalTok{)}

\CommentTok{\# create function for response at size 1000}
\NormalTok{run\_resp2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{  out2\_resp1 }\OtherTok{\textless{}{-}}\NormalTok{ out2 }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(\textbackslash{}(x) \{}
      \CommentTok{\# get dataset number }
\NormalTok{      no }\OtherTok{\textless{}{-}}\NormalTok{ x}\SpecialCharTok{$}\NormalTok{sim[}\DecValTok{1}\NormalTok{]}
\NormalTok{      x }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{|\textgreater{}} 
        \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(As}\SpecialCharTok{:}\NormalTok{Sn, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{c}\NormalTok{(}\FunctionTok{scale}\NormalTok{(.)))) }
\NormalTok{      df }\OtherTok{\textless{}{-}} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{100} \SpecialCharTok{\textasciitilde{}} \FunctionTok{base\_case}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{200} \SpecialCharTok{\textasciitilde{}} \FunctionTok{am1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{300} \SpecialCharTok{\textasciitilde{}} \FunctionTok{am2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{400} \SpecialCharTok{\textasciitilde{}} \FunctionTok{ap1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{500} \SpecialCharTok{\textasciitilde{}} \FunctionTok{ap2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{600} \SpecialCharTok{\textasciitilde{}} \FunctionTok{bm1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{700} \SpecialCharTok{\textasciitilde{}} \FunctionTok{bm2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{800} \SpecialCharTok{\textasciitilde{}} \FunctionTok{bp1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{900} \SpecialCharTok{\textasciitilde{}} \FunctionTok{bp2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1000} \SpecialCharTok{\textasciitilde{}} \FunctionTok{cm1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1100} \SpecialCharTok{\textasciitilde{}} \FunctionTok{cm2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1200} \SpecialCharTok{\textasciitilde{}} \FunctionTok{cp1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1300} \SpecialCharTok{\textasciitilde{}} \FunctionTok{cp2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1400} \SpecialCharTok{\textasciitilde{}} \FunctionTok{dm1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1500} \SpecialCharTok{\textasciitilde{}} \FunctionTok{dm2}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1600} \SpecialCharTok{\textasciitilde{}} \FunctionTok{dp1}\NormalTok{(x), }
\NormalTok{        no }\SpecialCharTok{\textless{}=} \DecValTok{1700} \SpecialCharTok{\textasciitilde{}} \FunctionTok{dp2}\NormalTok{(x), }
        \AttributeTok{.default =}\NormalTok{ x }\CommentTok{\#note 1701 {-} 2100 is for cov{-}exp interxn}
\NormalTok{      ) }
\NormalTok{    \}) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{set\_names}\NormalTok{(}\AttributeTok{nm =} \FunctionTok{c}\NormalTok{(}
      \FunctionTok{rep}\NormalTok{(}\StringTok{"\_base"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"am1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"am2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"ap1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"ap2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"bm1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"bm2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"bp1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"bp2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"cm1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"cm2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"cp1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"cp2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"dm1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"dm2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"dp1"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"dp2"}\NormalTok{, }\DecValTok{100}\NormalTok{), }
      \FunctionTok{rep}\NormalTok{(}\StringTok{"unset"}\NormalTok{, }\DecValTok{400}\NormalTok{)}
\NormalTok{    ))}
  \FunctionTok{return}\NormalTok{(out2\_resp1)}
\NormalTok{\}}

\CommentTok{\# send to HPC}
\NormalTok{runresplg }\OtherTok{\textless{}{-}} \FunctionTok{slurm\_call}\NormalTok{(}
\NormalTok{  run\_resp2, }
  \AttributeTok{global\_objects =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}out2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}base\_case\textquotesingle{}}\NormalTok{, }
                     \StringTok{\textquotesingle{}am1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}am2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ap1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ap2\textquotesingle{}}\NormalTok{, }
                     \StringTok{\textquotesingle{}bm1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bm2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bp1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bp2\textquotesingle{}}\NormalTok{, }
                     \StringTok{\textquotesingle{}cm1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cm2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cp1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cp2\textquotesingle{}}\NormalTok{, }
                     \StringTok{\textquotesingle{}dm1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dm2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dp1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dp2\textquotesingle{}}\NormalTok{),}
  \AttributeTok{jobname =} \StringTok{\textquotesingle{}sim\_resp2\textquotesingle{}}\NormalTok{)}

\CommentTok{\# get output}
\NormalTok{out2\_resp1 }\OtherTok{\textless{}{-}} \FunctionTok{get\_slurm\_out}\NormalTok{(runresplg)}
\CommentTok{\# only save output for exp{-}exp interxns for now}
\NormalTok{out2\_resp1 }\OtherTok{\textless{}{-}}\NormalTok{ out2\_resp1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1700}\NormalTok{]}
\FunctionTok{write\_rds}\NormalTok{(out2\_resp1, }\StringTok{"sim/sim\_resp\_lg\_a.RDS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#\#\#}
\CommentTok{\# create response variables for exposure{-}covariate interxn}
\DocumentationTok{\#\#\#\#\#\#\#\#\#\#\#\#\#}
\end{Highlighting}
\end{Shaded}
\hypertarget{code-for-chapter-refmodels}{%
\subsection{Code for Chapter \ref{models}:}\label{code-for-chapter-refmodels}}

Here, we fit the models to our simulated data. We use the \texttt{rslurm}, \texttt{bkmr}, and \texttt{NLinteraction} packages in this section. This code was run on the Amherst HPC RStudio server.

\hypertarget{code-for-chapter-refresults}{%
\subsection{Code for Chapter \ref{results}:}\label{code-for-chapter-refresults}}

Here, we extract results from our simulation. We use the \texttt{rslurm}, \texttt{bkmr}, and \texttt{NLinteraction} packages in this section. This code was run on the Amherst HPC RStudio server.

\hypertarget{corrections}{%
\chapter*{Corrections}\label{corrections}}
\addcontentsline{toc}{chapter}{Corrections}

A list of corrections after submission to department.

Corrections may be made to the body of the thesis, but every such correction will be acknowledged in a list under the heading ``Corrections,'' along with the statement ``When originally submitted, this honors thesis contained some errors which have been corrected in the current version. Here is a list of the errors that were corrected.'' This list will be given on a sheet or sheets to be appended to the thesis. Corrections to spelling, grammar, or typography may be acknowledged by a general statement such as ``30 spellings were corrected in various places in the thesis, and the notation for definite integral was changed in approximately 10 places.'' However, any correction that affects the meaning of a sentence or paragraph should be described in careful detail. The files samplethesis.tex and samplethesis.pdf show what the ``Corrections'' section should look like. Questions about what should appear in the ``Corrections'' should be directed to the Chair.

\backmatter

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\noindent

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-antonelli_estimating_2020}{}}%
Antonelli, J., Mazumdar, M., Bellinger, D., Christiani, D., Wright, R., \& Coull, B. (2020). Estimating the health effects of environmental mixtures using {Bayesian} semiparametric regression and sparsity inducing priors. \emph{The Annals of Applied Statistics}, \emph{14}(1), 257--275. http://doi.org/\href{https://doi.org/10.1214/19-AOAS1307}{10.1214/19-AOAS1307}

\leavevmode\vadjust pre{\hypertarget{ref-barrera-gomez_systematic_2017}{}}%
Barrera-Gómez, J., Agier, L., Portengen, L., Chadeau-Hyam, M., Giorgis-Allemand, L., Siroux, V., \ldots{} Basagaña, X. (2017). A systematic comparison of statistical methods to detect interactions in exposome-health associations. \emph{Environmental Health}, \emph{16}(1), 74. http://doi.org/\href{https://doi.org/10.1186/s12940-017-0277-6}{10.1186/s12940-017-0277-6}

\leavevmode\vadjust pre{\hypertarget{ref-bastain_study_2019}{}}%
Bastain, T. M., Chavez, T., Habre, R., Girguis, M. S., Grubbs, B., Toledo-Corral, C., \ldots{} Breton, C. (2019). Study {Design}, {Protocol} and {Profile} of the {Maternal} {And} {Developmental} {Risks} from {Environmental} and {Social} {Stressors} ({MADRES}) {Pregnancy} {Cohort}: A {Prospective} {Cohort} {Study} in {Predominantly} {Low}-{Income} {Hispanic} {Women} in {Urban} {Los} {Angeles}. \emph{BMC Pregnancy and Childbirth}, \emph{19}(1), 189. http://doi.org/\href{https://doi.org/10.1186/s12884-019-2330-7}{10.1186/s12884-019-2330-7}

\leavevmode\vadjust pre{\hypertarget{ref-bellavia_statistical_2021}{}}%
Bellavia, A. (2021). \emph{Statistical {Methods} for {Environmental} {Mixtures}}. Retrieved from \url{https://bookdown.org/andreabellavia/mixtures/preface.html}

\leavevmode\vadjust pre{\hypertarget{ref-bobb_introduction_2017}{}}%
Bobb, J. F. (2017a, March). Introduction to {Bayesian} kernel machine regression and the bkmr {R} package. Retrieved from \url{https://jenfb.github.io/bkmr/overview.html}

\leavevmode\vadjust pre{\hypertarget{ref-bobb_example_2017}{}}%
Bobb, J. F. (2017b, December). Example using the bkmr {R} package with simulated data from the {NIEHS} mixtures workshop. Retrieved from \url{https://jenfb.github.io/bkmr/SimData1.html\#1_load_packages_and_download_data}

\leavevmode\vadjust pre{\hypertarget{ref-bobb_statistical_2018}{}}%
Bobb, J. F., Claus Henn, B., Valeri, L., \& Coull, B. A. (2018). Statistical software for analyzing the health effects of multiple concurrent exposures via {Bayesian} kernel machine regression. \emph{Environmental Health}, \emph{17}(1), 67. http://doi.org/\href{https://doi.org/10.1186/s12940-018-0413-y}{10.1186/s12940-018-0413-y}

\leavevmode\vadjust pre{\hypertarget{ref-bobb_bayesian_2015}{}}%
Bobb, J. F., Valeri, L., Claus Henn, B., Christiani, D. C., Wright, R. O., Mazumdar, M., \ldots{} Coull, B. A. (2015). Bayesian kernel machine regression for estimating the health effects of multi-pollutant mixtures. \emph{Biostatistics}, \emph{16}(3), 493--508. http://doi.org/\href{https://doi.org/10.1093/biostatistics/kxu058}{10.1093/biostatistics/kxu058}

\leavevmode\vadjust pre{\hypertarget{ref-cribb_surviving_2016}{}}%
Cribb, J. (2016). \emph{Surviving the 21st century: {Humanity}'s ten great challenges and how we can overcome them}. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-genest_primer_2007}{}}%
Genest, C., \& Nešlehovà, J. (2007). A {Primer} on {Copulas} for {Count} {Data}. \emph{ASTIN Bulletin}, \emph{37}(2), 475--515. http://doi.org/\url{https://doi.org/10.2143/AST.37.2.2024077}

\leavevmode\vadjust pre{\hypertarget{ref-gibson_overview_2019}{}}%
Gibson, E. A., Nunez, Y., Abuawad, A., Zota, A. R., Renzetti, S., Devick, K. L., \ldots{} Kioumourtzoglou, M.-A. (2019). An overview of methods to address distinct research questions on environmental mixtures: An application to persistent organic pollutants and leukocyte telomere length. \emph{Environmental Health}, \emph{18}(1), 76. http://doi.org/\href{https://doi.org/10.1186/s12940-019-0515-1}{10.1186/s12940-019-0515-1}

\leavevmode\vadjust pre{\hypertarget{ref-halford_how_2005}{}}%
Halford, G. S., Baker, R., McCredden, J. E., \& Bain, J. D. (2005). How {Many} {Variables} {Can} {Humans} {Process}? \emph{Psychological Science}, \emph{16}(1), 70--76. http://doi.org/\href{https://doi.org/10.1111/j.0956-7976.2005.00782.x}{10.1111/j.0956-7976.2005.00782.x}

\leavevmode\vadjust pre{\hypertarget{ref-hastie_elements_2009}{}}%
Hastie, T., Tibshirani, R., \& Friedman, J. (2009). \emph{The {Elements} of {Statistical} {Learning}}. New York, NY: Springer New York. http://doi.org/\href{https://doi.org/10.1007/978-0-387-84858-7}{10.1007/978-0-387-84858-7}

\leavevmode\vadjust pre{\hypertarget{ref-hernandez_toxic_2013}{}}%
Hernández, A. F., Parrón, T., Tsatsakis, A. M., Requena, M., Alarcón, R., \& López-Guarnido, O. (2013). Toxic effects of pesticide mixtures at a molecular level: {Their} relevance to human health. \emph{Toxicology}, \emph{307}, 136--145. http://doi.org/\href{https://doi.org/10.1016/j.tox.2012.06.009}{10.1016/j.tox.2012.06.009}

\leavevmode\vadjust pre{\hypertarget{ref-heys_risk_2016}{}}%
Heys, K., Shore, R., Pereira, M., Jones, K., \& Martin, F. (2016). Risk assessment of environmental mixture effects. \emph{RSC Advances}, \emph{6}(53), 47844--47857. http://doi.org/\href{https://doi.org/10.1039/C6RA05406D}{10.1039/C6RA05406D}

\leavevmode\vadjust pre{\hypertarget{ref-hofert_copula_2023}{}}%
Hofert, M., Kojadinovic, I., Maechler, M., \& Yan, J. (2023). Copula: {Multivariate} {Dependence} with {Copulas}. Retrieved from \url{https://CRAN.R-project.org/package=copula}

\leavevmode\vadjust pre{\hypertarget{ref-hoskovec_model_2021}{}}%
Hoskovec, L., Benka-Coker, W., Severson, R., Magzamen, S., \& Wilson, A. (2021). Model choice for estimating the association between exposure to chemical mixtures and health outcomes: {A} simulation study. \emph{PLOS ONE}, \emph{16}(3), e0249236. http://doi.org/\href{https://doi.org/10.1371/journal.pone.0249236}{10.1371/journal.pone.0249236}

\leavevmode\vadjust pre{\hypertarget{ref-howe_prenatal_2020}{}}%
Howe, C. G., Claus, H. B., Eckel, S. P., Farzan, S. F., Grubbs, B. H., Chavez, T. A., \ldots{} Breton, C. V. (2020). Prenatal {Metal} {Mixtures} and {Birth} {Weight} for {Gestational} {Age} in a {Predominately} {Lower}-{Income} {Hispanic} {Pregnancy} {Cohort} in {Los} {Angeles}. \emph{Environmental Health Perspectives}, \emph{128}(11), 117001. http://doi.org/\href{https://doi.org/10.1289/EHP7201}{10.1289/EHP7201}

\leavevmode\vadjust pre{\hypertarget{ref-kannan_exposures_2006}{}}%
Kannan, S., Misra, D. P., Dvonch, J. T., \& Krishnakumar, A. (2006). Exposures to {Airborne} {Particulate} {Matter} and {Adverse} {Perinatal} {Outcomes}: {A} {Biologically} {Plausible} {Mechanistic} {Framework} for {Exploring} {Potential} {Effect} {Modification} by {Nutrition}. \emph{Environmental Health Perspectives}, \emph{114}(11), 1636--1642. http://doi.org/\href{https://doi.org/10.1289/ehp.9081}{10.1289/ehp.9081}

\leavevmode\vadjust pre{\hypertarget{ref-kordas_interactions_2007}{}}%
Kordas, K., Lönnerdal, B., \& Stoltzfus, R. J. (2007). Interactions between nutrition and environmental exposures: Effects on health outcomes in women and children. \emph{The Journal of Nutrition}, \emph{137}(12), 2794--2797. http://doi.org/\href{https://doi.org/10.1093/jn/137.12.2794}{10.1093/jn/137.12.2794}

\leavevmode\vadjust pre{\hypertarget{ref-lazarevic_performance_2020}{}}%
Lazarevic, N., Knibbs, L. D., Sly, P. D., \& Barnett, A. G. (2020). Performance of variable and function selection methods for estimating the nonlinear health effects of correlated chemical mixtures: {A} simulation study. \emph{Statistics in Medicine}, \emph{39}(27), 3947--3967. http://doi.org/\href{https://doi.org/10.1002/sim.8701}{10.1002/sim.8701}

\leavevmode\vadjust pre{\hypertarget{ref-liu_semiparametric_2007}{}}%
Liu, D., Lin, X., \& Ghosh, D. (2007). Semiparametric {Regression} of {Multidimensional} {Genetic} {Pathway} {Data}: {Least}-{Squares} {Kernel} {Machines} and {Linear} {Mixed} {Models}. \emph{Biometrics}, \emph{63}(4), 1079--1088. http://doi.org/\href{https://doi.org/10.1111/j.1541-0420.2007.00799.x}{10.1111/j.1541-0420.2007.00799.x}

\leavevmode\vadjust pre{\hypertarget{ref-muller_weighted_1987}{}}%
Müller, H.-G. (1987). Weighted {Local} {Regression} and {Kernel} {Methods} for {Nonparametric} {Curve} {Fitting}. \emph{Journal of the American Statistical Association}, \emph{82}(397), 231--238. http://doi.org/\href{https://doi.org/10.1080/01621459.1987.10478425}{10.1080/01621459.1987.10478425}

\leavevmode\vadjust pre{\hypertarget{ref-nadaraya_estimating_1964}{}}%
Nadaraya, E. A. (1964). On {Estimating} {Regression}. \emph{Theory of Probability \& Its Applications}, \emph{9}(1), 141--142. http://doi.org/\href{https://doi.org/10.1137/1109020}{10.1137/1109020}

\leavevmode\vadjust pre{\hypertarget{ref-naidu_chemical_2021}{}}%
Naidu, R., Biswas, B., Willett, I. R., Cribb, J., Kumar Singh, B., Paul Nathanail, C., \ldots{} Aitken, R. J. (2021). Chemical pollution: {A} growing peril and potential catastrophic risk to humanity. \emph{Environment International}, \emph{156}, 106616. http://doi.org/\href{https://doi.org/10.1016/j.envint.2021.106616}{10.1016/j.envint.2021.106616}

\leavevmode\vadjust pre{\hypertarget{ref-national_academies_of_sciences_engineering_and_medicine_using_2017}{}}%
National Academies of Sciences, Engineering, and Medicine, Division on Earth and Life Studies, Board on Environmental Studies and Toxicology, \& Committee on Incorporating 21st Century Science into Risk-Based Evaluations. (2017). \emph{Using 21st {Century} {Science} to {Improve} {Risk}-{Related} {Evaluations}}. Washington, D.C.: National Academies Press. http://doi.org/\href{https://doi.org/10.17226/24635}{10.17226/24635}

\leavevmode\vadjust pre{\hypertarget{ref-nelsen_introduction_2006}{}}%
Nelsen, R. B. (2006). \emph{An introduction to copulas} (2nd ed). New York: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-persson_outside_2022}{}}%
Persson, L., Carney Almroth, B. M., Collins, C. D., Cornell, S., De Wit, C. A., Diamond, M. L., \ldots{} Hauschild, M. Z. (2022). Outside the {Safe} {Operating} {Space} of the {Planetary} {Boundary} for {Novel} {Entities}. \emph{Environmental Science \& Technology}, \emph{56}(3), 1510--1521. http://doi.org/\href{https://doi.org/10.1021/acs.est.1c04158}{10.1021/acs.est.1c04158}

\leavevmode\vadjust pre{\hypertarget{ref-pesenti_comparative_2023}{}}%
Pesenti, N., Quatto, P., Colicino, E., Cancello, R., Scacchi, M., \& Zambon, A. (2023). Comparative efficacy of three {Bayesian} variable selection methods in the context of weight loss in obese women. \emph{Frontiers in Nutrition}, \emph{10}, 1203925. http://doi.org/\href{https://doi.org/10.3389/fnut.2023.1203925}{10.3389/fnut.2023.1203925}

\leavevmode\vadjust pre{\hypertarget{ref-plackett_quantal_1952}{}}%
Plackett, R. L., \& Hewlett, P. S. (1952). Quantal {Responses} to {Mixtures} of {Poisons}. \emph{Journal of the Royal Statistical Society: Series B (Methodological)}, \emph{14}(2), 141--154. http://doi.org/\href{https://doi.org/10.1111/j.2517-6161.1952.tb00108.x}{10.1111/j.2517-6161.1952.tb00108.x}

\leavevmode\vadjust pre{\hypertarget{ref-r_core_team_r_2013}{}}%
R Core Team. (2013). \emph{R: A language and environment for statistical computing: Reference index}. Vienna: R Foundation for Statistical Computing.

\leavevmode\vadjust pre{\hypertarget{ref-schulz_tutorial_2018}{}}%
Schulz, E., Speekenbrink, M., \& Krause, A. (2018). A tutorial on {Gaussian} process regression: {Modelling}, exploring, and exploiting functions. \emph{Journal of Mathematical Psychology}, \emph{85}, 1--16. http://doi.org/\href{https://doi.org/10.1016/j.jmp.2018.03.001}{10.1016/j.jmp.2018.03.001}

\leavevmode\vadjust pre{\hypertarget{ref-siemiatycki_biological_1981}{}}%
Siemiatycki, J., \& Thomas, D. C. (1981). Biological {Models} and {Statistical} {Interactions}: An {Example} from {Multistage} {Carcinogenesis}. \emph{International Journal of Epidemiology}, \emph{10}(4), 383--387. http://doi.org/\href{https://doi.org/10.1093/ije/10.4.383}{10.1093/ije/10.4.383}

\leavevmode\vadjust pre{\hypertarget{ref-sun_statistical_2013}{}}%
Sun, Z., Tao, Y., Li, S., Ferguson, K. K., Meeker, J. D., Park, S. K., \ldots{} Mukherjee, B. (2013). Statistical strategies for constructing health risk models with multiple pollutants and their interactions: Possible choices and comparisons. \emph{Environmental Health}, \emph{12}(1), 85. http://doi.org/\href{https://doi.org/10.1186/1476-069X-12-85}{10.1186/1476-069X-12-85}

\leavevmode\vadjust pre{\hypertarget{ref-taylor_statistical_2016}{}}%
Taylor, K. W., Joubert, B. R., Braun, J. M., Dilworth, C., Gennings, C., Hauser, R., \ldots{} Carlin, D. J. (2016). Statistical {Approaches} for {Assessing} {Health} {Effects} of {Environmental} {Chemical} {Mixtures} in {Epidemiology}: {Lessons} from an {Innovative} {Workshop}. \emph{Environmental Health Perspectives}, \emph{124}(12), A227--A229. http://doi.org/\href{https://doi.org/10.1289/EHP547}{10.1289/EHP547}

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele_tutorial_2014}{}}%
VanderWeele, T. J., \& Knol, M. J. (2014). A {Tutorial} on {Interaction}. \emph{Epidemiologic Methods}, \emph{3}(1), 33--72. http://doi.org/\href{https://doi.org/10.1515/em-2013-0005}{10.1515/em-2013-0005}

\leavevmode\vadjust pre{\hypertarget{ref-vineis_john_2018}{}}%
Vineis, P. (2018). From {John} {Snow} to omics: The long journey of environmental epidemiology. \emph{European Journal of Epidemiology}, \emph{33}(4), 355--363. http://doi.org/\href{https://doi.org/10.1007/s10654-018-0398-4}{10.1007/s10654-018-0398-4}

\leavevmode\vadjust pre{\hypertarget{ref-wagaman_probability_2021}{}}%
Wagaman, A. S., \& Dobrow, R. P. (2021). \emph{Probability: {With} {Applications} and {R}} (1st ed.). Wiley. http://doi.org/\href{https://doi.org/10.1002/9781119692430}{10.1002/9781119692430}

\leavevmode\vadjust pre{\hypertarget{ref-ward_how_2019}{}}%
Ward, J. B., Gartner, D. R., Keyes, K. M., Fliss, M. D., McClure, E. S., \& Robinson, W. R. (2019). How do we assess a racial disparity in health? {Distribution}, interaction, and interpretation in epidemiological studies. \emph{Annals of Epidemiology}, \emph{29}, 1--7. http://doi.org/\href{https://doi.org/10.1016/j.annepidem.2018.09.007}{10.1016/j.annepidem.2018.09.007}

\leavevmode\vadjust pre{\hypertarget{ref-watson_smooth_1964}{}}%
Watson, G. S. (1964). Smooth {Regression} {Analysis}. \emph{Sankhyā: The Indian Journal of Statistics}, \emph{26}(4), 359--372.

\leavevmode\vadjust pre{\hypertarget{ref-yu_review_2022}{}}%
Yu, L., Liu, W., Wang, X., Ye, Z., Tan, Q., Qiu, W., \ldots{} Chen, W. (2022). A review of practical statistical methods used in epidemiological studies to estimate the health effects of multi-pollutant mixture. \emph{Environmental Pollution}, \emph{306}, 119356. http://doi.org/\href{https://doi.org/10.1016/j.envpol.2022.119356}{10.1016/j.envpol.2022.119356}

\end{CSLReferences}
% Index?

\end{document}
