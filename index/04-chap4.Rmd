---
output:
  pdf_document: default
  html_document: default
---
# Simulations {#sims}

```{r include_packages2, include = FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
```

## Past simulation studies

Here, we preface our simulation with an overview of examples in the literature which compare various methods for exposure mixture studies using simulations. @taylor_statistical_2016 conclude that, in general for exposure mixture studies, no single method consistently outperforms others across all situations and, importantly, that a method should be chosen based on the question of interest. Thus, for each study, we highlight not only the findings, but also the data-generating scenarios and the identified question of interest. 

@lazarevic_performance_2020 compare the performance of a broad range of methods for accurate variable selection of important exposures. They simulated exposure data using a multivariate copula based on real-world data and the response by specifying a regression model with only a subset of truly significant exposures and a normal error term. Two correlation structures were considered — one with the original Spearman correlation matrix and one with the values halved — as well as two signal-to-noise ratios — one with an $R^2$ for the true model at 10\% and one at 30\%. They found that BKMR, along with three other flexible regression methods that allow for nonlinearity, provided more accurate variable selection results compared to two machine learning methods. Moreover, they observed that, in general, low signal-to-noise ratios had a stronger impact on performance than did increasing multicollinearity. 

@hoskovec_model_2021 compare Bayesian methods, including BKMR, while considering 4 research questions: accurate estimation, selection of important exposures, exclusion of unimportant exposures, and identification of interactions. They use observed exposure and covariate data to simulate response data using regression relationships; they considered three exposure-response scenarios of varying complexity and included two-way multiplicative interaction terms. For each simulated dataset, they randomly assigned exposures to be active components of the mixture to incorporate variability in the data. Overall, they found that Bayesian methods outperformed traditional linear regressions, and that BKMR performed best when the exposure-response function takes on a complex form. 

Most recently, @pesenti_comparative_2023 compare BKMR, BSR, and the Bayesian Least Absolute Shrinkage and Selection Operator (LASSO) for variable selection. Data were generated using a multivariate normal with moderate and strong correlation structures specified manually by the researchers. They found that, in situations with additive and linear exposure-response relationships, Bayesian LASSO was appropriate. Across the other scenarios, BKMR generally performed best, while BSR selected exposures with high heterogeneity when the sample size was smaller due to the influence of the degrees of freedom, $d$, tuning parameter. Notably, multicollinearity did not generally lead to spurious variable selection. 

Finally, we briefly comment on studies by @sun_statistical_2013 and @barrera-gomez_systematic_2017, whose explicit goal is to compare methods for identifying interactions. Both studies generate exposure data using the correlation structure from an existing dataset; @sun_statistical_2013 uses a multivariate lognormal, while @barrera-gomez_systematic_2017 uses a multivariate normal. Both only consider two-way, multiplicative interactions. While neither of these studies consider the methods used in this thesis, they find that, in general, models that formally allow for interaction effects perform better than models that only allow for univariate additive effects. 

## Methods

The goal of our simulation study is to provide guidance on the choice between BSR and BKMR for characterizing a diverse range of complex interactions between predictors. In particular, we aim to extend findings from previous simulation studies by considering a more comprehensive range of interaction types, including different effect sizes, non-multiplicative interactions, and three-way interactions. We also explore interactions between exposures and categorical covariates, a previously understudied form of interaction in exposure mixture studies. 

### MADRES data {#madres}

In order to make our simulations comparable to real-world exposure mixture studies, we based our simulation data on the Maternal And Developmental Risks from Environmental and Social Stressors (MADRES) pregnancy cohort. The MADRES cohort is an ongoing, prospective pregnancy cohort of predominantly lower-income, Hispanic women in Los Angeles, California, which began in 2015 [@bastain_study_2019]. Urine samples were collected by participants at their first visit, and questionnaires were administered during their first visit, with follow-ups at the first, second, and third trimesters. See @bastain_study_2019 for further details on study design. 

@howe_prenatal_2020 previously examined the effect of prenatal metal mixtures of birth weight (BW) for gestational age (GA) in this cohort. They used BKMR to identify associations between metal mixtures and BW for GA, as well as BSR to conduct inference on interactions between metals. Briefly, using BKMR, they found that, of the metals in the mixture, mercury and nickel were most strongly associated with BW for GA. Moreover, BKMR results suggested that a potential interaction between mercury and nickel exists; however, when run through BSR, the PIP for this interaction was extremely small, despite being the highest of all two-way interactions. 

Data from the study by @howe_prenatal_2020 were obtained from publicly available data in the Human Health Exposure Resource (HHEAR) Data Repository, which has been approved under Icahn School of Medicine at Mount Sinai IRB Protocol #16-00947. The Digital Object Identifiers associated with the urinary trace element data and epidemiological data are 10.36043/1945_159 and 10.36043/1945_177, respectively. All analyses were conducted in R v4.3.2 [@r_core_team_r_2013].

```{r logtransf, fig.cap="Distributions of original (a) and natural log transformed (b) concentrations of metals in MADRES cohort (n=252).", out.width = '90%', echo = FALSE}
include_graphics(path = "figures/ch4_univlog.png")
```


We followed the approach by @howe_prenatal_2020 for preparing the data for analysis. This resulted in retaining 10 metals in analysis: arsenic (As), cadmium (Cd), cobalt (Co), mercury (Hg), nickel (Ni), molybdenum (Mo), lead (Pb), antimony (Sb), tin (Sn), and thallium (Tl). @howe_prenatal_2020 used speciated As, but this was not available in HHEAR, so we used total As. Metals were expressed in nanograms per milliliter (ng/mL) and natural log transformed to reduce right-skewness (Figure \@ref(fig:logtransf)). Among the full range of covariates considered by @howe_prenatal_2020, we used the subset of 4 that were available in HHEAR: any smoke exposure during pregnancy, maternal prepregnancy body mass index (BMI), maternal age during firt trimester, and maternal race by ethnicity and birth place. We chose not to include study site, as there was a study site with only 1 participant. Race by ethnicity and birth place was collapsed into the following categories: non-Hispanic white, non-Hispanic black, non-Hispanic other, Hispanic born in the US, and Hispanic born outside the US. We observed 8 missing values for BMI in the data from HHEAR, which were not reported by @howe_prenatal_2020. We mean imputed these missing values. Distributions of covariates are shown in Figure \@ref(fig:covdist). Our final analytic dataset included 252 participants, which was 10 fewer than in @howe_prenatal_2020, likely due to small discrepancies in their dataset and the one made available in HHEAR. 

```{r covdist, fig.cap="Distributions of continuous (a) and categorical (b) covariates in the MADRES cohort (n=252).", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch4_covdist.png")
```


### Using copulas to simulate predictor data {#copula}

We simulated exposure and covariate data (hereafter referred to collectively as predictors) using a multivariate Gaussian copula fit on the 252 participants in the MADRES cohort. We used copulas as they can preserve both the correlation structure and marginal distributions from the observed data, allowing us to replicate conditions in a real-world scenario. 

First, we briefly introduce copulas in the context of their use in this simulation, based on the presentation in @nelsen_introduction_2006. Copulas are joint cumulative distribution functions (CDFs) defined on the unit cube $[0,1]^n$ that capture the dependence between $n$ uniformly distributed marginals. Sklar's theorem allows us to apply copulas to our observed data. Sklar's theorem states that, if $H(x_1, \dots x_n)$ is a joint CDF of the marginal CDFs $F_1(x_1), \dots, F_n(x_n)$, then there exists a copula $C$ such that, for all $(x_1, \dots, x_n)$ in $(X_1, \dots, X_n)$, 

$$
H(x_1, \dots x_n)=C(F_1(x_1), \dots, F_n(x_n)).
$$

\noindent Note that, by the probability integral transform, or the universality of the uniform, the CDFs $F_1(x_1), \dots, F_n(x_n)$ are distributed uniformly. 

We used the `copula` package in R to fit copulas and generate random data [@hofert_copula_2023]. We transformed the observed continuous predictor values to uniform distributions based on their empirical marginal CDFs, a process called generating "pseudo-random" samples. We used the checkerboard copula approach for generating pseudo-random samples for smoke exposure, a binary variable [@genest_primer_2007]. We coded smoke exposure as 0's and 1's, generated a pseudo-random sample, and then "jittered" the values with uniform random noise. There is currently no widely accepted approach for generating pseudo-random samples from unordered categorical variables with more than two levels. Thus, we excluded race by ethnicity and birthplace from the copula model. While this means that our simulated datasets did not preserve any potential association between race and exposures, Figure \@ref(fig:raceexp) suggests that there is little to no visible association between race and exposures in the observed dataset. 

```{r raceexp, fig.cap="Association between race by ethnicity and birth place and metal exposures in the MADRES cohort (n=252).", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch4_race_exp.png")
```

Various families of copulas have been described, each of which specifies a different shape for the dependence structure. We performed model selection to identify the copula that best approximates the dependence structure of our data. We fit the set of multivariate copulas used by @lazarevic_performance_2020 in their simulation study, which included the Gaussian, $t$, Gumbel, Frank, Clayton, and Joe copulas. We fit two $t$ copulas with 4 and 10 degrees of freedom, which controls dependence at the tails of the distributions, as well as a $t$ copula where the degrees of freedom was determined during the fitting process. The Gumbel, Frank, Clayton, and Joe copulas require a $\theta$ parameter, which controls dependence between the distributions. We fit two versions of these copulas with $\theta=\{2, 4\}$. Among these, the Gaussian copula minimized the Akaike information criterion and maximized the likelihood, so we proceeded with this model. The Gaussian copula assumes a bivariate normal dependence structure between the marginal CDFs. 

```{r univexpsim, fig.cap="Distributions of log-transformed exposures from observed data (blue) and 2100 simulated smaller size (n=252) datasets (gray).", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch4_univ_exp_sim.png")
```


```{r univcovsim, fig.cap = "Distributions of continuous (a) and categorical (b) covariates from observed data (blue) and 2100 simulated smaller size (n=252) datasets (gray).", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch4_univ_cov_sim.png")
```

We simulated predictor data by randomly sampling from the fitted multivariate Gaussian copula distribution. All pseudo-random samples were then back-transformed to their original distributions using empirical marginal CDFs. We simulated the race by ethnicity and birthplace variable by randomly assigning observations to each of the five categories based on proportions in the observed dataset. 

```{r corsimssm, fig.cap = "Spearman's correlation heat maps of exposures from observed data (a) and averaged from 2100 smaller size (n=252) simulated datasets (b).", out.width = '100%', echo = FALSE}
include_graphics(path = "figures/ch4_corr_sim+orig.png")

# , as well as distribution of correlations from smaller size simulated datasets (c)
```

We generated one set of simulated datasets with the same sample size as the observed dataset (n=252), which is typical in many cohort studies. We also generated another set of simulated datasets with a larger sample size (n=1000), which has become increasingly common with the rise of larger-scale studies. The goal of this choice was to inform sample size considerations in study design. We verified that the original structure of the observed dataset were preserved by visually comparing univariate distributions of exposures (Figure \@ref(fig:univexpsim)) and covariates (Figure \@(fig:univcovsim)), as well as the correlation structure using Spearman's $\rho$ (Figure \@ref(fig:corsimssm)). Distributions of Spearman's correlation were approximately normal (Figure \@ref(fig:cordistsm)). Plots for the larger size simulated datasets were similar (Figures \@ref(fig:univexplg), \@ref(fig:univcovlg), and \@ref(fig:corsimslg)). 

### Simulating predictor-response relationships {#simresp}

Health outcome responses were simulated under several different scenarios, each of which included different effect sizes and functional forms for the interactions. All scenarios were run for both the smaller (n=252) and larger (n=1000) sample sizes. In the first scenario, we specified a "base case" model:

\begin{multline*}
Y_i = \textrm{Hg}_i + \frac{3}{1+\textrm{exp}(-4\textrm{Ni}_i)} + \frac{1.5}{1+\textrm{exp}(-4\textrm{Sn}_i)} - \textrm{Sb}_i^2 + 0.5\textrm{Sb}_i\\
+ \textrm{age}_i + 0.5\textrm{bmi}_i + 0.5\textrm{race}_{\textrm{black}i} + 0.5\textrm{race}_{\textrm{hisp.non}i} + 1.5\textrm{smoke}_i + \varepsilon_i,
\end{multline*}

\noindent where $\varepsilon_i \overset{\mathrm{iid}}{\sim} N(0,5)$. This model includes a linear term for Hg, two S-shaped logistic terms for Ni and Sn with varying effect sizes, and a symmetric inverse U-shaped quadratic term for Sb (Figure \@ref(fig:univlines)). Moreover, we included covariate terms as linear effects in the model. We chose the standard deviation on the normal random error term in order to achieve an $R^2$ of around 0.1-0.3 in a multiple linear regression that included only the true functional form of the significant chemicals (Figure \@ref(fig:rsqcheck)). This $R^2$ range approximates realistic signal-to-noise ratios in exposure mixture studies [@lazarevic_performance_2020]. 

In subsequent scenarios, we added an additional interaction term to the base case model. First, we considered interactions between two exposures. We defined four cases of interest: a two-way interaction between exposures that are univariately significant, a two-way interaction between exposures that are univariately insignificant, a two-way interaction between exposures that are moderately collinear, and a three-way interaction. For each case, we considered two functional forms — multiplicative and polynomial — and a lower and higher effect size, which we set by defining the weight on the interaction term in the model. The higher effect sizes were selected in order to achieve a power of approximately 0.5 at $\alpha=0.05$ in the smaller sample size (n=252) case, using a multiple linear regression with the true functional form of the chemicals specified and the covariate terms included. The lower effect sizes were set equal to half of the higher effect size. Table \@ref(tab:scenarios) shows the specification of interaction terms. See Appendix \@ref(suppmethods), Figures \@ref(fig:basesurf)-\@ref(fig:cp2), for 3D surfaces of the two-way interaction terms. 

```{r scenarios, echo = FALSE}
equations <- data.frame(
  type = rep(c("Multiplicative", "Polynomial"), 4), 
  small = c("0.35Hg$*$Ni", "0.13Hg$*($Ni$-1)^2$", 
            "0.35Cd$*$As", "0.125Cd$*($As$-1)^2$", 
            "0.3Ni$*$Co", "0.1Ni$*($Co$-1)^2$", 
            "0.3Hg$*$Ni$*$Tl", "0.09Hg$*($Ni$-1)^2*$Tl"), 
  large = c("0.7Hg$*$Ni", "0.26Hg$*($Ni$-1)^2$", 
            "0.7Cd$*$As", "0.25Cd$*($As$-1)^2$", 
            "0.6Ni$*$Co", "0.2Ni$*($Co$-1)^2$", 
            "0.6Hg$*$Ni$*$Tl", "0.18Hg$*($Ni$-1)^2*$Tl")
)
labels <- c(
  "Univariately significant" = 2, 
  "Univariately insignificant" = 2, 
  "Highly correlated" = 2, 
  "Three-way interaction" = 2
)

equations |> 
  kbl(booktabs = TRUE, escape = FALSE, 
      col.names = c("", "Lower", "Higher"), 
      align = "lcc",
      caption = "Specification of interaction terms in simulations.") |> 
  column_spec(1, width = "10em") |> 
  add_header_above(header = c(" " = 1, "Effect size" = 2)) |>
  pack_rows(index = labels) 
```

Next, we considered interactions between the race by ethnicity and birthplace covariate (hereafter referred to as race for concision) and an exposure. We are interested in cases where the health effects of an exposure are higher in one group compared to the rest. In a real-world scenario, such interactions can arise from excess amounts of social stress experienced by a group due to racism. To model this, we increased the coefficient Hg in Non-Hispanic Black individuals (n=27 in the original MADRES cohort) for the first scenario, and in Hispanic individuals born outside the US (n=109 in the original MADRES cohort) for the second scenario. The goal of this choice was to assess the impact of group size on detectability of an interaction, and to quantify the potential value of oversampling the minority group. For each scenario, we specified a lower effect size by increasing the coefficient on Hg by $1.5\times$ (i.e. from $1*$Hg to $1.5*$Hg) in the target group, and a higher effect size by increasing the coefficient on Hg by $2\times$ (i.e. from $1*$Hg to $2*$Hg). 

This resulted in a total of 42 scenarios ([1 base case + 5 interaction cases $\times$ 2 effect sizes $\times$ 2 functional forms] $\times$ 2 sample sizes = 42). For each scenario, we generated 100 simulated datasets, resulting in a total of 4200 datasets. 

<!-- We hypothesize that... [*add hypothesis*] -->

### Models {#models}

We ran four methods on our simulated datasets. All metal concentrations and continuous covariates were standardized in analysis to keep values scale-free. 

To get a baseline, we ran a multiple linear regression, including all exposures and covariates as linear, additive terms in the model. We refer to this model as the naive MLR. Then, we ran a multiple linear regression with the true model explicitly specified by excluding non-significant exposures and specifying the known form of non-linear terms and non-additive interactions. We refer to this model as the oracle MLR. In scenarios with an interaction between race and Hg, we collapsed race into a binary variable indicating whether or not the original race category was interacting with Hg before running oracle MLR's, in order to simplify the detection of the interaction. 

Next, we ran BKMR using the `bkmr` package in R [@bobb_statistical_2018; @bobb_bkmr_2022]. We chose to implement component-wise variable selection rather than hierarchical selection to make simulation results more interpretable, and because there was only moderate multicollinearity in the observed and simulated data. We specified the default priors [@bobb_bayesian_2015, and as listed in Chapter \@ref(bkmrprior)], which is common in the literature for BKMR [e.g., @lazarevic_statistical_2019, @howe_prenatal_2020, @pesenti_comparative_2023]. We ran the MCMC sampler for 50,000 iterations, as recommended by @bobb_statistical_2018, and discarded the first 25,000 iterations for burn-in. BKMR does not provide the option to run multiple chains or to thin chains. For larger size datasets, we sped up computations by employing a Gaussian predictive process on 100 knots specified evenly across the predictor space. 

We ran BSR using the `NLinteraction` package in R [@antonelli_nlinteraction_2018]. We specified the default priors [@antonelli_estimating_2020, and as listed in Chapter \@ref(bsrprior)], which is common in the literature for BSR [e.g., @howe_prenatal_2020; @pesenti_comparative_2023]. @antonelli_estimating_2020 suggests separately fitting models for degrees of freedom $d=\{1, 2, 3, 4\}$ and selecting the value for $d$ which minimizes WAIC. Due to time constraints in this thesis, we first fit BSR on the grid of values for $d$ using 5,000 MCMC iterations to obtain the empirical Bayes estimate for $\sigma^2_{\boldsymbol\beta}$ and then another 5,000 MCMC iterations to obtain the posterior distributions, discarding the first 2,500 iterations for burn-in each time. We selected $d$ based on the WAIC criterion on these preliminary models. Then, we fit the full BSR model using 50,000 MCMC iterations to obtain the empirical Bayes estimate and then another 50,000 MCMC iterations to obtain the posterior distributions, discarding the first 25,000 iterations for burn-in each time. We ran two chains to very convergence, thinning each chains by selecting every 8th iteration to reduce autocorrelation based on default settings. In a small test run on five smaller size datasets for each scenario containing interactions between exposures, as well as the base case, we found that using 5,000 iterations selected the same degrees of freedom as using 50,000 iterations 86\% of the time (see Appendix \@ref(suppmethods), Figure \@ref(fig:comparedf)). 

Finally, we ran stratified BKMR and BSR models in scenarios where we simulated an interaction between race and Hg. This involved running five separate models for each race category, each with the same settings specified above. For the smaller size datasets, we often observed convergence issues in BKMR within the smaller race categories. As such, for smaller size datasets, we also assess the impact of collapsing the three smaller race categories (Non-Hispanic white, black, and other) into one category before stratifying. This is a common practice in real-world studies where sample sizes for certain categories are low. 

<!-- - expect that there will be less power to detect these interactions, especially in smaller size datasets -->

We checked convergence for a selection of BKMR and BSR models using trace plots (Appendix \@ref(suppresults), Figure \@ref(fig:traceplots)). 

### Model assessment 

We assessed model performance based on detection of significant univariate chemicals as well as detection of interactions. For the naive and oracle MLRs, we considered a $p$-value less than 0.05 to indicate detection of a significant term. For BKMR and BSR, we used the median probability model, which considers a PIP greater than or equal to 0.5 to indicate detection of a significant term [@barbieri_optimal_2004]. 

While BSR provides PIP's to quantify detection of interactions, BKMR does not. As such, for BKMR, we considered formal detection of an interaction based on confidence intervals constructed around the estimated response. Specifically, we first calculated the difference in estimated response at a chemical's 0.25 and 0.75 quantiles. Then, we assessed whether this quantity differed at the 0.25 and 0.75 quantiles of one (or two, for three-way interactions) other chemicals in the interaction, while holding all other chemicals at their 0.5 quantiles, by constructing a 95\% confidence interval of the difference in differences. We followed the code in the `SingVarIntSummaries()` function in the `bkmr` package for constructing confidence intervals [@bobb_bkmr_2022]. 

For both BKMR and BSR, we also visually assessed detection of interactions by plotting the estimated exposure-response surface for one chemical while fixing one (or two, for three-way interactions) other chemicals at their 0.1, 0.5, and 0.9 quantiles. In all scenarios, we calculated the sensitivity as the proportion of times a significant term was correctly detected. Due to time constraints in this thesis, we only calculated the false discovery rates, or, the proportion of times a significant term is incorrectly detected, for two-way interactions between chemicals. Future work should include the calculation of false discovery rates for all interactions. 

Finally, for stratified models, we constructed a 95\% [*do we have to do adjustment for multiple comparisons?*] confidence interval for the difference in estimated response at the 0.25 and 0.75 quantiles of Hg, for both BKMR and BSR on each subcategory of race. We refer to this as the estimated magnitude of response. Then, we considered an interaction as detected if at least one of the pair-wise comparisons between estimated magnitudes of response differed between two categories of race. For instance, if the confidence intervals for the estimated magnitude of response were all (-1, 1) for the first four categories, but the fifth category had a confidence interval of (2,  4), we would consider this to be a significant interaction. 

[*still need to figure out whether we can do this for bsr*]

Finally, we also visualized the estimated exposure-response relationship for Hg in each of the stratified models. 


## Results {#results}

### Base case

We start by presenting results from models run on the base case scenario, in which the true relationship contained no interactions. Figure \@ref(fig:basecasesig) displays the distribution of p-values and PIPs from this scenario, while Table \@ref(tab:basecasetab) summarizes the overall sensitivities and false discovery rates based on these values. Note that insignificant chemicals were not included in the oracle MLR, which is why their distributions are omitted from this output. 

The naive MLR does the best job at picking up the effects of Hg, with a sensitivity of 0.8 and 1 in the smaller and larger size datasets, respectively. This is likely because Hg is the only linear term in the model. Ni and Sn have an S-shaped curve with higher and lower effect sizes, respectively, so the naive MLR detects a slight linear signal from them. Sb, which has a U-shaped curve, is the hardest to pick up. On the other hand, the oracle MLR consistently detects Hg, Ni, and Sb. The smaller size oracle MLRs only occasionally pick up Sn, likely due to the lower effect sisze. 

This base case scenario confirms that the multiple regression and Bayesian models behave as expected. We also recognize that, for unvariate significance metrics, BKMR tends to produce higher false discovery rates

```{r basecasesig, fig.cap = "P-value distributions from smaller (a) and larger (b) size datasets and PIP distributions from smaller (c) and larger (d) size datasets.", out.width = '100%', echo = FALSE}
include_graphics(path = "figures/ch4_basecasesig.png")
```

```{r basecasetab, echo = FALSE, message = FALSE}
base_sens <- read_csv("data/base_sens.csv")

base_sens2 <- base_sens |> 
  mutate(var = gsub("[^[:alpha:]]","", var), 
         mod = factor(mod, levels = c("Naive MLR", "Oracle MLR", "BKMR", "BSR"))) |> 
  arrange(desc(sign), var, mod, desc(size)) |> 
  select(-sign) |> 
  pivot_wider(names_from = var, values_from = sensitivity)

base_mod_ord <- table(base_sens2$mod)

options(knitr.kable.NA = '-')
base_sens2 |> 
  select(-mod) |> 
  kbl(booktabs = TRUE, escape = FALSE, 
      col.names = c("", names(base_sens2)[3:12]),
      align = "lcccccccccc",
      caption = "Sensitivity and false discovery rate (FDR) of chemicals in base case scenario.") |> 
  column_spec(1, width = "6em") |>
  add_header_above(header = c(" " = 1, "Sensitivity" = 4, "FDR" = 6)) |>
  pack_rows(index = base_mod_ord) 
```


### Univariate sensitivity 

Hg Ni full output

```{r hgniuniv, fig.cap = "P-value (a) and PIP (b) distributions for univariate chemicals from all models in scenarios with an interaction between Hg and Ni.", out.width = '100%', echo = FALSE}
include_graphics("figures/ch4_hgni_univ.png")
```

Table of sensitivity and FDR across all models

```{r onewaytab, echo = FALSE, message = FALSE}
naive_sens <- read_csv("data/naive_sens.csv")
oracle_sens <- read_csv("data/oracle_sens.csv")
bkmr_sens <- read_csv("data/bkmr_pip_sens.csv")
bsr_sens <- read_csv("data/bsr_pip_sens.csv")

comb_sens <- bind_rows(
  rename(naive_sens, variable = var),
  filter(select(oracle_sens,-var), variable != "Int"),
  mutate(bkmr_sens, mod = "BKMR"),
  mutate(bsr_sens, mod = "BSR")
) |>
  filter(case != 1) |>
  mutate(
    variable = gsub("[^[:alpha:]]", "", variable),
    mod = factor(mod, levels = c("Naive MLR", "Oracle MLR", "BKMR", "BSR")) ,
    sign = ifelse(mod == "Oracle MLR", TRUE, sign),
    new_sign = factor(ifelse(
      variable %in% c("Hg", "Ni", "Sb", "Sn"),
      "Significant",
      "Insignificant"
    ))
  ) |>
  group_by(mod, size, new_sign) |>
  summarize(detection = mean(sensitivity)) |> 
  pivot_wider(names_from = new_sign, values_from = detection) |> 
  arrange(mod, desc(size)) |> 
  ungroup()

comb_sens_ord <- table(comb_sens$mod)

options(knitr.kable.NA = '-')
comb_sens |> 
  select(-mod) |> 
  relocate(size, Significant, Insignificant) |> 
  kbl(booktabs = TRUE, escape = FALSE, digits = 3, align = "lcc",
      caption = "Sensitivity and false discovery rate (FDR) of chemicals in all scenarios with two-way interactions between exposures.", 
      col.names = c("", "Sensitivity", "FDR")) |> 
  column_spec(1, width = "6em") |>
  pack_rows(index = comb_sens_ord) 
```

Everything else is in appendix

### Two-way interactions between chemicals

Probably only include false discovery rates for BKMR for, again, Hg and Ni to reduce complexity and run-time, note that future work would involve getting FDR's for all models

Hg Ni full output

```{r hgnibiv, echo = FALSE, out.width = '100%'}
include_graphics("figures/ch4_hgni_biv.png")
```


Table of all sensitivities

```{r twowaytab, echo = FALSE, message = FALSE}
oracle_comb <- read_csv("data/oracle_sens.csv")
bkmr_comb <- read_csv("data/bkmr_int_sens.csv")
bsr_comb <- read_csv("data/bsr_int_sens.csv")

comb_int <- bind_rows(
  oracle_comb |> filter(variable == "Int") |> 
    select(-variable, cond = var), 
  bkmr_comb |> mutate(mod = "BKMR"), 
  bsr_comb |> mutate(mod = "BSR") |> filter(sign) |> rename(cond = inter2) |> select(-sign)
) |>
  filter(case %in% 2:13) |> 
  mutate(cond = gsub(":", "-", cond), 
         cond = gsub("\\+", "-", cond),
         cond = case_when(
           cond == "I(Hg * ((Ni - 1)^2))" ~ "Hg-Ni", 
           cond == "I(Cd * ((As - 1)^2))" ~ "Cd-As", 
           cond == "I(Ni * ((Co - 1)^2))" ~ "Ni-Co", 
           cond == "As-Cd" ~ "Cd-As", 
           cond == "Co-Ni" ~ "Ni-Co", 
           .default = cond
         ), 
         cond = factor(cond, levels = c("Hg-Ni", "Cd-As", "Ni-Co")), 
         inter_type = ifelse(case %% 4 %in% 1:2, "Multiplicative", "Polynomial"), 
         effect_size = factor(ifelse(case %% 2 == 1, "Higher", "Lower"), 
                              levels = c("Lower", "Higher"))) |> 
  select(-case) |>
  arrange(desc(size)) |> 
  pivot_wider(names_from = c(mod, size), values_from = sensitivity, 
              names_sep = " ") |> 
  arrange(cond, inter_type, effect_size)

comb_int_ord <- table(comb_int$cond)

comb_int |> 
  select(-cond) |> 
  kbl(booktabs = TRUE, escape = FALSE, 
      align = "llcccccc", 
      caption = "Sensitivity to interactions in all scenarios with two-way interactions between exposures.", 
      col.names = c("Interaction type", "Effect size", rep(c("Oracle", "BKMR", "BSR"), 2))) |> 
  add_header_above(header = c(" " = 2, "Small" = 3, "Large" = 3), bold = TRUE) |>
  pack_rows(index = comb_int_ord) |> 
  collapse_rows(columns = 1, valign = "middle", latex_hline = "linespace")
```

Table of FDRs

```{r twowayfdrtab, echo = FALSE, message = FALSE}
fdr_comb <- read_csv("data/comb_int_fdr.csv")

fdr_comb2 <- fdr_comb |>
  filter(inter2 == "none", case %in% 2:13) |>
  mutate(
    cond = case_when(
      case %in% 2:5 ~ "Hg-Ni",
      case %in% 6:9 ~ "Cd-As",
      case %in% 10:13 ~ "Ni-Co"
    ),
    cond = factor(cond, levels = c("Hg-Ni", "Cd-As", "Ni-Co")), 
    inter_type = ifelse(case %% 4 %in% 1:2, "Multiplicative", "Polynomial"),
    effect_size = factor(
      ifelse(case %% 2 == 1, "Higher", "Lower"),
      levels = c("Lower", "Higher")
    )
  ) |> 
  select(-inter2, -sign, -case) |>  
  pivot_wider(names_from = c(mod, size), values_from = sensitivity) |> 
  arrange(cond, inter_type, effect_size) |> 
  relocate(cond, inter_type, effect_size, BKMR_Small, BSR_Small, BKMR_Large, BSR_Large)

fdr_ord <- table(fdr_comb2$cond)

fdr_comb2 |> 
  select(-cond) |> 
  kbl(booktabs = TRUE, escape = FALSE, 
      align = "llcccc", 
      caption = "False discovery rate of interactions in all scenarios with two-way interactions between exposures.", 
      col.names = c("Interaction type", "Effect size", rep(c("BKMR", "BSR"), 2))) |> 
  add_header_above(header = c(" " = 2, "Small" = 2, "Large" = 2), bold = TRUE) |>
  pack_rows(index = fdr_ord) |> 
  collapse_rows(columns = 1, valign = "middle", latex_hline = "linespace")
```


Everything else is in appendix


### Three-way interactions between chemicals

Decide whether or not to include a figure???

Table of sensitivities (try alternative ways of thresholding BSR???)

```{r threewaysens, echo = FALSE, message = FALSE}
triv <- read_csv("data/triv_sens.csv")

triv |> 
  select(-case) |> 
  mutate(sensitivity = round(sensitivity, 2)) |> 
  pivot_wider(names_from = c(mod, size), values_from = sensitivity) |> 
  relocate(1, 2, 4, 6, 8, 3, 5, 7) |> 
  kbl(booktabs = TRUE, 
      caption = "Sensitivity to trivariate.", 
      col.names = c("Interaction type", "Effect size", rep(c("Oracle", "BKMR", "BSR"), 2))) |> 
  add_header_above(header = c(" " = 2, "Small" = 3, "Large" = 3), bold = TRUE) |>
  collapse_rows(columns = 1, valign = "middle", latex_hline = "linespace")
```

Consider getting false discovery rates? 

### Interactions between race and an exposure

blah

### Run-time analysis

```{r runtimes, echo = FALSE, message = FALSE}
times <- read_csv("data/time1.csv")

times |> 
  kbl(booktabs = TRUE, 
      align = "llcccc", 
      caption = "Run-times.", 
      col.names = c("Model", "Sample size", "Base", "Lower", "Higher", "Lower", "Higher")) |> 
  add_header_above(header = c(" " = 3, "Multiplicative" = 2, "Polynomial" = 2), bold = TRUE) |>
  # pack_rows(index = comb_int_ord) |> 
  collapse_rows(columns = 1, valign = "middle", latex_hline = "linespace")
```

## Discussion

Discuss results, link back to previous sim studies. 

