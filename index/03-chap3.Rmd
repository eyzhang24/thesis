---
output:
  pdf_document: default
  html_document: default
---
```{r, include = FALSE}
library(knitr)
```

# Bayesian regression methods {#bayes}

## Motivation

We are interested in using Bayesian regression techniques to characterize the nature of complex interactions. In addition to being an interesting statistical challenge, complex interactions are also relevant from both a mechanistic and public health point of view. 

-   mechanistic
-   public health

Drawing from the mechanistic and public health contexts, there are three classes of interactions that are of interest. 

-   different types of interactions

However, non-additive interactions are difficult to detect. 

First, there are different forms that an interaction can take on. For instance, in the linear regression setting, the form of the interaction must be explicitly specified; most commonly, this is done by multiplying two variables together. However, 

Moreover, the number of possible interactions to consider quickly becomes intractable in high-dimensional settings. For instance, consider modelling 10 exposures in a linear regression setting. In order to be assessed, each interaction must be explicitly specified as a new term in the model. Including all the possible two-way interactions would involve adding ${10 \choose 2} = 45$ additional terms to the model, and all possible three-way interactions would add ${10 \choose 3} = 120$ additional terms. 

-   interactions are difficult to pick up
-   consider the number of interactions that would have to be encoded explicitly in MLR

It is important also to acknowledge, here, that there is a limit to how many variables can be included in an interaction before it becomes incomprehensible to most humans. 


## Bayesian kernel machine regression

Notation for kernel machine regression:

<!-- -   observations are from $i = 1, \dots ,n$ -->
-   $\textbf{x}_m$ is a predictor variable in the predictor matrix $\textbf{X}$ with $m = 1, \dots, M$, measuring exposure variables or covariates in this case
-   $\textbf{x}_i$ is a vector of values for a single observation in $\textbf{X}$ with $i = 1, \dots, n$
-   $x_{im}$ is an observation of $\textbf{x}_m$
-   $Y_i$ is an observation of $\textbf{Y}$, measuring the health outcome in this case
-   $h(\textbf{x}_i)$ is the flexible function relating X to Y, represented by the kernel
-   $k$ is the kernel function, the Gaussian in this case
-   $\textbf{K}$ is the $n \times n$ kernel matrix, with $(i, j)$th element $k(\textbf{x}_i, \textbf{x}_j)$
-   $\rho$ is the tuning parameter inside the kernel function which controls smoothness
-   $\tau$ is the tuning parameter multiplied by the kernel matrix to represent covariance between $h$ values
-   $\boldsymbol\epsilon_i \overset{\mathrm{iid}}{\sim} N(0, \sigma^2)$ are the residuals of the response

BKMR specific notation: 

-   $r_m=1/\rho_m$ is an augmented variable in $\textbf{r}$ in the kernel matrix for controlling smoothness
-   $\delta_m$ is an indicator variable in $\boldsymbol\delta$ which represents inclusion in the model
-   $\mathcal{S}_g$ is a group of partitioned predictors with ${g=1,\dots,G}$
-   $\{\delta_m|\textbf{x}_m \in{\mathcal{S}_g}\}$ is an indicator variable in $\boldsymbol\delta_{\mathcal{S}_g}$ which represents inclusion of a parameter in group $g$ in the model 
-   $\pi$ is the prior probability of inclusion in the model
-   $\lambda = \tau\sigma^{-2}$ is a convenient way to define the prior on $\tau$


### Kernel machine regression

In this section, we introduce kernel machine regression, with attention to its specific implementation in BKMR. First proposed by Nadaraya [-@nadaraya_estimating_1964] and Watson [-@watson_smooth_1964], kernel machine regression is a nonparametric regression technique that can be used to capture nonlinear effects and nonadditive interactions. In this introduction, we follow the presentation of kernel machine regression provided by Bobb et al. [-@bobb_bayesian_2015]. 

To contextualize this method, we start at the typical linear regression setting,

$$
Y_i = \textbf{x}_i^\top\boldsymbol{\beta} + \epsilon_i,
$$

\noindent where $Y_i$ measures a health outcome at a given point, $\textbf{x}_i = (x_{1},\dots,x_{M})^\top$ is a vector of M exposures or covariates (hereafter referred to as predictors), $\boldsymbol{\beta}$ is a vector of weights, and $\epsilon_i$ is a random variable from $\boldsymbol\epsilon \overset{\mathrm{iid}}{\sim} N(0, \sigma^2)$. We can see that this function assumes that there is a linear relationship between the exposure and the response, and that the combined effects of multiple exposures are additive. 

Kernel machine regression defines this relationship using a function $h: \mathbb{R}^M \rightarrow \mathbb{R}$, where

$$
Y_i = h(\textbf{x}_i) + \epsilon_i.
$$

\noindent Here, $h(\cdot)$ is represented by the function $k(\cdot, \cdot)$, a kernel. The kernel controls the covariance, or the similarity, between values of $h(\textbf{x})$ and as such ensures that points near each other on the prediction surface will have similar values — or, in other words, that the prediction surface will be smooth. In the case of kernel machine regression, we define a positive definite kernel where $k: \mathbb{R}^M\times \mathbb{R}^M \rightarrow \mathbb{R}$. 

There are many choices of functions for $k$. BKMR uses the Gaussian kernel, also known as the radial basis function or, sometimes, the squared exponential kernel. The Gaussian kernel is defined as

<!-- $$ -->
<!-- h(\textbf{x}) = \sum_{i=1}^n\alpha_i k(\textbf{x}_i, \textbf{x}) -->
<!-- $$ -->

<!-- for some set of constants $\{\alpha_i\}_{i=1}^n$, where $\textbf{x}$ is a vector of exposure values, and $\textbf{x}_i$ is the exposure profile of a subject in the dataset. Now, in order to obtain a value for $h$, we need to solve for $\alpha_i$ and $k$.  -->

$$
k(\textbf{x}, \textbf{x}') = \textrm{exp}\{
-\frac{||\textbf{x}-\textbf{x}'||^2}{\rho}\},
$$

\noindent where $||\textbf{x}-\textbf{x}'||^2 = \sum_{m=1}^M{(x_{m}-x_{m}')^2}$ for a set of predictors values $\textbf{x}$ and the predictor values of a second subject $\textbf{x}'$, and $\rho$ is a tuning parameter that controls the relationship between the correlation between two points and their distance. Greater values of $\rho$ will enforce more dependence between points and make the resulting function smoother. $h$ is related to $k$ by a multiplicative constant $\tau$, a tuning parameter which controls the vertical scale of $h$.

Now that we have defined $h$ and $k$, we can think about how to characterize the relationship between our response and predictors. Kernel machine regression is a nonparametric technique because it does not specify a functional form for this relationship. Hence, we will think about estimating the response at a particular query point. Operationally, Müller [-@muller_weighted_1987] demonstrated that kernel machine regression uses a weighted average of all the observations in the dataset to estimate the response, defined as

$$
\bar{Y} = \frac{\sum_{i=1}^nw_iY_i}{\sum_{i=1}^nw_i},
$$

\noindent with some set of weights $\{w_i\}_{i=1}^n$. Intuitively, we want to weight the observations that are closer to the query point more heavily. Using the Gaussian kernel as a weight allows us to achieve this. Replacing the weight with the Gaussian kernel, we get

$$
\bar{Y} = \frac{\sum_{i=1}^n k(\textbf{x}, \textbf{x}_i) Y_i}
{\sum_{i=1}^n k(\textbf{x}, \textbf{x}_i)}.
$$

As we move through the predictor space, we can think of the prediction as a continuous moving average of local points in the dataset. The correlation between two values of $h$ is defined as

$$
\textrm{cor}(h_i, h_j) = \textrm{exp}\{-\frac{\sum_{m=1}^M
{(x_{im}-x_{jm})^2}}{\rho}\},
$$

\noindent which allows us to see that values of $h$ near each other will have a higher correlation and thus similar values. This is also why the resulting function is smooth. 

### Connection with mixed models

It is useful to make connections between this definition of kernel machine regression and mixed models. Liu et al. [-@liu_semiparametric_2007] demonstrated this by representing $h(\textbf{x})$ as following a Gaussian process probability distribution, 

$$
h(\textbf{x}) \sim \mathcal{GP}(\textbf{0}, \tau k(\textbf{x}, \textbf{x}')),
$$

\noindent with mean function $m$ and covariance function $k$, where $\textbf{x}$ is a vector of the predictor values, and $\textbf{x}'$ contains the predictor values of another subject. A Gaussian process is a collection of random variables, of which any finite number follow a multivariate normal distribution [@schulz_tutorial_2018]. Here, we assume that the expected value of the $h$ function with input $\textbf{x}$ is $\textbf{0}$. We use $k$ for the covariance function, which represents the dependence between the function values with inputs $\textbf{x}$ and $\textbf{x}'$: $k(\textbf{x}, \textbf{x}') = \mathbb{E}[(h(\textbf{x})- \textbf{0}) (h(\textbf{x}')- \textbf{0})]$. 

Now, we can represent $h$ as a collection of variables from a Gaussian process. $h$ follows a multivariate normal distribution,

$$
h({\textbf{x}}) \sim N(\textbf{0}, \tau\textbf{K}),
$$

\noindent where $h({\textbf{x}}) = [h(\textbf{x}_1), h(\textbf{x}_2), \dots, h(\textbf{x}_n)]^\top$ and $\textbf{K}$ is the kernel matrix. The kernel matrix is an $n \times n$ matrix with $(i, j)$th element $k(\textbf{x}_i, \textbf{x}_j)$. Now, returning back to the regression view, we can think of each $Y_i$ as following the distribution,

$$
Y_i \overset{\mathrm{ind}}{\sim} N(h(\textbf{x}_i), \sigma^2) \text{ for } i = 1,\dots,n,
$$

\noindent where $\sigma^2$ comes from the variance of the residuals. 

### Toy example

In the following section, we introduce kernel machine regression with a toy example.

Consider the following case where we want to model the relationship between a single predictor and a response variable. Suppose the true relationship between $x$ and $Y$ is defined $Y = e^{\frac{x}{10}} + 2\sin(\frac{x}{2})$. 

```{r toy1, fig.cap = "Nonlinear data with a true distribution (orange) and a fitted linear regression (blue).", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch3_toy1.png")
```

Figure \@ref(fig:toy1) illustrates the shape of our simulated nonlinear data and the fit proposed by a simple linear regression. We can observe that the linear regression fails to capture the true nonlinear relationship. In this case, this would lead to an underestimation of the true association between $x$ and $Y$. Now, we will try to capture this relationship using kernel machine regression. 

To visualize how kernel machine regression works as a moving weighted average, we can consider a query point of 12.5. 

```{r toy2, fig.cap="A query point of 12.5 and the weights of neighboring observations based on a Gaussian kernel", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch3_toy2.png")
```

Figure \@ref(fig:toy2) identifies the query point and assigns corresponding weights to the neighboring points based on a normal distribution, which shares the same density as the Gaussian kernel. In this case, we will specify $\rho = 2$, which is synonymous with assigning the weights using a normal distribution with $\sigma^2=1$. We can see how an appropriate estimate for $h(12.5)$ can be obtained by taking a weighted average of the $x$'s, with those observations nearby weighted the most heavily. 

<!-- Note to self, bandwidth represent four times the quartiles of the probability density. In the case of a Gaussian kernel, the bandwidth represents $\frac{8}{3}\sigma$, so using an sd of 1 translates to bandwidth of 8/3 -->

Now, we fit a kernel machine regression on this data with $\rho=2$ using the `stats` package in R [@r_core_team_r_2013]. 

```{r toy3, fig.cap="Fitted kernel machine regression with $\\rho=2$.", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch3_toy3.png")
```

We can see in Figure \@ref(fig:toy3) that kernel machine regression captures the complex nonlinear relationship between $Y$ and $x$ and closely follows the true distribution. We do note, though, that the estimation is less precise at the tails, where there is less information provided by local observations. We can also use this example to consider the effect of various values of $rho$ on the smoothness of the $h$ function. 

```{r toyrho, fig.cap="Fitted kernel machine regression with $\\rho=0.02$ and $\\rho=50$.", out.width = '100%', echo = FALSE}
include_graphics(path = "figures/ch3_toyrho.png")
```

Figure \@ref(fig:toyrho) demonstrates the effect of relatively smaller and larger values of $\rho$ on $h$. Decreasing the value of $\rho$ allows kernel machine regression to overfit to the noise in the data by relaxing the dependence of neighboring values of $h$ to each other. On the other hand, increasing the value of $\rho$ enforces more dependence in $h$ and as such results in an underfit estimation. Hence, the choice of $\rho$ has a strong effect on the performance of kernel machine regression. 

### Variable selection

Now that we have defined kernel machine regression, we can extend it to the Bayesian paradigm. Bobb et al. [-@bobb_bayesian_2015] showed that the Bayesian approach outperforms frequentist kernel machine regression because simultaneous variable selection and estimation can better capture the exposure-response relationship. In this section, we discuss the two methods for Bayesian variable selection in BKMR: hierarchical variable selection and component-wise variable selection [@bobb_bayesian_2015].

Component-wise selection follows the same framework as variable selection in a typical Bayesian multiple regression except, instead of augmenting each predictor, we augment the kernel function as 

$$
k(\textbf{x}, \textbf{x}'|\textbf{r}) = \text{exp}\{ -\sum_{m=1}^Mr_m(x_m-x_m')^2\},
$$

\noindent where $\textbf{r}=[r_1,\dots,r_M]^\top$. We define $r_m=1/{\rho_m}$, the inverse of the tuning parameter $\rho_m$ for each $\textbf{x}_m$, as we want $r_m$ to be able to take on the value of $0$ when a variable is removed during the selection process. We now define the kernel matrix $\textbf{K}_{\textbf{X},\textbf{r}}$ as the $n\times n$ matrix with $(i,j)$th element $k(\textbf{x}, \textbf{x}'|\textbf{r})$. To allow $r_m$ to equal 0 with non-zero probability, we first define an indicator variable determining whether or not a predictor is included in the variable, which is distributed as

$$
\delta_m \sim \text{Bernoulli}(\pi),
$$

\noindent where $\pi$ is the prior probability of inclusion. Now, we can assume a "slab-and-spike" prior on $r_m$, distributed as

$$
r_m|\delta_m \sim \delta_mf(r_m) + (1-\delta_m)P_0,
$$

\noindent where $f(\cdot)$ is some pdf with support $\mathbb{R}^+$ and $P_0$ denotes the density with point mass at 0.

While this process of component-wise variable selection works well in a typical multiple regression setting, it can lead to unreliable estimates in situations where the predictors are highly correlated with each other, which is common in exposure mixture studies. In this case, the correlated components contribute similar information to the model, and component-wise variable selection is not able to distinguish which predictor is important. BKMR deals with this problem by introducing hierarchical variable selection. 

Hierarchical variable selection involves partitioning the predictors $\textbf{x}_1, \dots, \textbf{x}_M$ a priori into groups $\mathcal{S}_g$ with $g = 1,\dots,G$. These groups should be selected with the aim of keeping within-group correlation high and between-group correlation low. The indicators from $r_m|\delta_m$ are now distributed as 

$$
\boldsymbol\delta_{\mathcal{S}_g} | \omega_g \sim 
\text{Multinomial}(\omega_g, \boldsymbol\pi_{\mathcal{S}_g}), g=1,\dots,G,\\
\omega_g \sim\text{Bernoulli}(\pi),
$$

\noindent where $\boldsymbol\delta_{\mathcal{S}_g}=\{\delta_m|\textbf{x}_m \in{\mathcal{S}_g}\}$ and $\boldsymbol\pi_{\mathcal{S}_g}$ are vectors of indicator variables and prior probabilities, respectively, of a predictor $\textbf{x}_m$ in group $\mathcal{S}_g$ entering the model. By this approach, at most one predictor in each group is allowed to enter the model. 

While hierarchical variable selection resolves the issue of multicollinearity, it requires specifying subgroups of predictors a priori and assumes that one predictor in each group can capture the information of the rest. Hence, care should be taken to justify the partitioning of predictors when taking this approach. 

Note also that the posterior means of $\delta_m$ generated from these variable selection procedures represent the posterior probability of inclusion of $\textbf{x}_m$. We can interpret these posterior "inclusion probabilities" as measures of the relative importance of each predictor. These measures can be used to understand the contribution of each exposure or covariate to the health outcome of interest in the model.  

### Prior specification 

In this section, we specify the prior distributions and parameters used by the BKMR algorithm [@bobb_bayesian_2015]. 

BKMR, by default, assumes $\rho_m=1/r_m \sim \text{Unif}(a_r,b_r)$, a flat prior between $a_r$ and $b_r$ for which the default values are 0 and 100, respectively [@bobb_introduction_2017]. This defines the prior probability of $\rho$ as equally distributed across any value from 0 to 100. This inverse of this prior corresponds to the slab component of the "slab-and-spike" prior, where $r_m|\delta_m \sim \delta_m\text{Unif}^{-1}(a_r, b_r) + (1-\delta_m)P_0$. As a flat prior, this distribution should be chosen when we have no prior knowledge about the smoothness of the exposure-response function, with hyperparameters $a_r$ and $b_r$ selected to represent the range of values we expect $\rho$ to potentially span. 

We have seen that kernel machine regression responds strongly to different values of $r_m=1/\rho$, and, accordingly, the model fit of BKMR is sensitive to their prior distribution. In general, the posterior inclusion probabilities generated from the variable selection procedure are particularly sensitive to this prior, though their relative importance tends to remain stable [@bobb_statistical_2018]. As such, the BKMR algorithm also offers the options to define uniform and gamma priors for the $r_m=1/\rho$. 

Moreover, BKMR assumes that the prior probability of including a predictor ($\delta_m$) or group of predictors ($\omega_g$) in the model is distributed $\pi \sim \text{Beta}(a_\pi, b_\pi)$. The default hyperparameters are $a_\pi=b_\pi=1$, which represent a flat, uninformative prior between 0 and 1. When the hierarchical selection approach is applied, equal values for $\boldsymbol\pi_{\mathcal{S}_g}$, the probabilities of inclusion for each component in group $\mathcal{S}_g$, are assumed.

Finally, BKMR assumes that the inverse of the variance of the residuals is distributed $\sigma^{-2} \sim \text{Gamma}(a_\sigma, b_\sigma)$, with default values of $a_\sigma=b_\sigma=0.001$, and that the vertical scale of $h$ is parameterized by $\lambda=\tau\sigma^{-2} \sim \text{Gamma}(a_\lambda, b_\lambda)$, with default values of $a_\lambda, b_\lambda$ such that the mean and variance of $\lambda$ are both equal to 10. 

### The MCMC algorithm

Briefly, we discuss the algorithm used in the BKMR package [@bobb_bayesian_2015; @bobb_statistical_2018], with commentary on its implications for the model fitting process. 

BKMR uses a Markov chain Monte Carlo (MCMC) algorithm with a mix of Gibbs and Metropolis-Hastings samplers to estimate the posterior distributions. In particular, a Gibbs step is used to update the distribution of $\sigma^2$ while a Metropolis-Hastings step is used to update the distribution of $\lambda$. For component-wise and hierarchical variable selection, $(\textbf{r}, \boldsymbol\delta, \boldsymbol\omega)$ are sampled jointly using a Metropolis-Hastings sampling scheme. 

While each distribution generated by the Gibbs step is always accepted, the distributions for $\lambda$ and $r_m$ generated by the Metroplis-Hastings steps are accepted based on an acceptance rate [@wagaman_probability_2021]. The standard deviation of the proposal distribution controls the acceptance rate and as such acts as a tuning parameter [@bobb_example_2017]. This standard deviation is a tuning parameter; in general, increasing the standard deviation leads to lower acceptance rates. Acceptance rates that are too low lead to slower convergence, but rates that are too high can cause convergence to an non-optimal distribution. 

A major computational limitation of BKMR is that at each iteration of the MCMC algorithm, the $n \times n$ augmented kernel matrix $\textbf{K}_{\textbf{Z},\textbf{r}}$ must be inverted multiple times. BKMR employs a Gaussian predictive process which involves specifying a set $l$ points, or "knots," that is a subset of the predictor space. The vector of predictors can be approximated by projection on this lower dimensional space, which allows the algorithm to perform inversions on an $l\times l$ matrix. 

## Bayesian semiparametric regression

differences w/ bkmr

-   makes distributional assumptions about the dataset
-   kernel regression is computationally intensive w/ large datasets but can handle many predictors
-   bsr highly dependent on the choice of function 

### Connections to linear mixed model

### Toy example

```{r toy4, fig.cap="Linear spline regression", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch3_toy4.png")
```

```{r toy5, fig.cap="Spline regression", out.width = '75%', echo = FALSE}
include_graphics(path = "figures/ch3_toy5.png")
```

### Sparsity inducing priors

### Prior specification

### The MCMC algorithm

## Detecting interactions

Discuss options for detecting interactions

BKMR can visualize relationship at various quantiles of other predictor, can conduct inference on inter-quantile difference

BSR can conduct inference

Potentially provide toy example 
